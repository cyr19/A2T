,label
ID: 970,
Abstract:,
"This paper proposes anew approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (""dev"") set. This profile might, for instance, be a vector with a di-mensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular sub-corpus to all the phrase pairs that can be extracted from the dev set. Then, for each phrase pair extracted from the training data, we create a vector with features defined in the same way, and calculate its similarity score with the vector representing the dev set. Thus, we obtain a decoding feature whose value represents the phrase pair's closeness to the dev. This is a simple, computationally cheap form of instance weighting for phrase pairs. Experiments on large scale NIST evaluation data show improvements over strong base-lines: +1.8 BLEU on Arabic to English and +1.4 BLEU on Chinese to English over a non-adapted baseline, and significant improvements inmost circumstances over baselines with linear mixture model adaptation. An informal analysis suggests that VSM adaptation may help in making a good choice among words with the same meaning, on the basis of style and genre.",
Introduction:,
"The translation models of a statistical machine translation (SMT) system are trained on parallel data. Usage of language and therefore the best translation practice differs widely across genres, topics, and dialects, and even depends on a particular author's or publication's style; the word ""domain"" is often used to indicate a particular combination of all these factors. Unless there is a perfect match between the training data domain and the (test) domain in which the SMT system will be used, one can often get better performance by adapting the system to the test domain.Domain adaptation is an active topic in the natural language processing (NLP) research community. Its application to SMT systems has recently received considerable attention. Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, instance weighting, and phrase sense disambiguation, etc.Research on mixture models has considered both linear and log-linear mixtures. Both were studied in, which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (, instead, opted for combining the sub-models directly in the SMT log-linear framework.In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (.Data selection approaches () search for bilingual sentence pairs that are similar to the indomain ""dev"" data, then add them to the training data.Instance weighting approaches () 1285 typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For example, a sentence from a subcorpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this subcorpus that appear to be of a general nature might receive higher weights.The 2012 JHU workshop on Domain Adaptation for MT 1 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system to find the appropriate translation.In this paper, we propose anew instance weighting approach to domain adaptation based on a vector space model (VSM). As in, this approach works at the level of phrase pairs. However, the VSM approach is simpler and more straightforward. Instead of using word-based features and a computationally expensive training procedure, we capture the distributional properties of each phrase pair directly, representing it as a vector in a space which also contains a representation of the dev set. The similarity between a given phrase pair's vector and the dev set vector becomes a feature for the decoder. It rewards phrase pairs that are in some sense closer to those found in the dev set, and punishes the rest. In initial experiments, we tried three different similarity functions: Bhattacharyya coefficient, Jensen-Shannon divergency, and cosine measure. They all enabled VSM adaptation to beat the non-adaptive baseline, but Bhattacharyya similarity worked best, so we adopted it for the remaining experiments.The vector space used by VSM adaptation can be defined in various ways. In the experiments described below, we chose a definition that measures the contribution (to counts of a given phrase pair, or to counts of all phrase pairs in the dev set) of each training subcorpus. Thus, the variant of VSM adaptation tested here bears a superficial resemblance to domain adaptation based on mixture models for TMs, as in, in that both approaches rely on information about the subcorpora from which the data originate. However, a key difference is that in this paper we explicitly capture each phrase pair's distribution across subcorpora, and compare it to the aggregated distribution of phrase pairs in the dev set. In mixture models, a phrase pair's distribu-tion across subcorpora is captured only implicitly, by probabilities that reflect the prevalence of the pair within each subcorpus. Thus, VSM adaptation occurs at a much finer granularity than mixture model adaptation. More fundamentally, there is nothing about the VSM idea that obliges us to define the vector space in terms of subcorpora.For instance, we could cluster the words in the source language into S clusters, and the words in the target language into T clusters. Then, treating the dev set and each phrase pair as a pair of bags of words (a source bag and a target bag) one could represent each as a vector of dimension S + T, with entries calculated from the counts associated with the S + T clusters (in away similar to that described for phrase pairs below). The (dev, phrase pair) similarity would then be independent of the subcorpora. One can think of several other ways of defining the vector space that might yield even better results than those reported here. Thus, VSM adaptation is not limited to the variant of it that we tested in our experiments.",
Conclusion:,
"This paper proposed anew approach to domain adaptation in statistical machine translation, based on vector space models (VSMs). This approach measures the similarity between a vector representing a particular phrase pair in the phrase table and a vector representing the dev set, yielding a feature associated with that phrase pair that will be used by the decoder. The approach is simple, easy to implement, and computationally cheap. For the two language pairs we looked at, it provided a large performance improvement over a non-adaptive baseline, and also compared 1291   favourably with linear mixture adaptation techniques. Furthermore, VSM adaptation can be exploited in a number of different ways, which we have only begun to explore. In our experiments, we based the vector space on subcorpora defined by the nature of the training data. This was done purely out of convenience: there are many, many ways to define a vector space in this situation. An obvious and appealing one, which we intend to try in future, is a vector space based on a bag-of-words topic model. A feature derived from this topicrelated vector space might complement some features derived from the subcorpora which we explored in the experiments above, and which seem to exploit information related to genre and style.",
,
Title 1: Domain Adaptation for SMT using a Vector Space Model,1
Title 2: Instance Weighting for SMT Model Adaptation based on a Vector Space Model,
,
,
ID: 1262,
Abstract:,
"Given the great amount of definite noun phrases that introduce an entity into the text for the first time, this paper presents a set of linguistic features that can be used to detect this type of definites in Span-ish. The efficiency of the different features is tested by building a rule-based and a learning-based chain-starting classifier. Results suggest that the classifier, which achieves high precision at the cost of recall , can be incorporated as either a filter or an additional feature within a corefer-ence resolution system to boost its performance .",
Introduction:,
"Although often treated together, anaphoric pronoun resolution differs from coreference resolution). Whereas the former attempts to find an antecedent for each anaphoric pronoun in a discourse, the latter aims to build full coreference chains, namely linking all noun phrases (NPs) -whether pronominal or with a nominal head -that point to the same entity. The output of anaphora resolution 1 are nounpronoun pairs (or pairs of a discourse segment and a pronoun in some cases), whereas the output of coreference resolution are chains containing a variety of items: pronouns, full NPs, discourse segments... Thus, coreference resolution requires a wider range of strategies in order to build the full chains of coreferent mentions. One of the problems specific to coreference resolution is determining, once a mention is encountered by the system, whether it refers to an entity previously mentioned or it introduces anew entity into the text. Many algorithms do not address this issue specifically, but implicitly assume all mentions to be potentially coreferent and examine all possible combinations; only if the system fails to link a mention with an already existing entity, it is considered to be chain starting. However, such an approach is computationally expensive and prone to errors, since natural language is populated with a huge number of entities that appear just once in the text. Even definite NPs, which are traditionally believed to refer to old entities, have been demonstrated to start a coreference chain over 50% of the times).An alternative line of research has considered applying a filter prior to coreference resolution that classifies mentions as either chain starting or coreferent. and have tested the impact of such a detector on the overall coreference resolution performance with encouraging results. Our chain-starting classifier is comparable -despite some differences -to the detectors suggested by,, and for English, but not identical to strictly anaphoric ones, since a non-anaphoric NP can corefer with a previous mention.This paper presents a corpus-based study of def-inite NPs in Spanish that results in a set of eight features that can be used to identify chain-starting definite NPs. The heuristics are tested by building two different chain-starting classifiers for Spanish, a rule-based and a learning-based one. The evaluation gives priority to precision over recall in view of the classifier's efficiency as a filtering module. The paper proceeds as follows. Section 2 provides a qualitative comparison with related work. The corpus study and the empirically driven set of heuristics for recognizing chain-starting definites are described in Section 3. The chain-starting classifiers are builtin Section 4. Section 5 reports on the evaluation and discusses its implications. Finally, Section 6 summarizes the conclusions and outlines future work.",
Conclusion:,
"The paper presented a corpus-driven chainstarting classifier of definite NPs for Spanish, pointing out and empirically supporting a series of linguistic features to betaken into account. Given that definiteness is very much language de-pendent, the AnCora-Es corpus was mined to infer some linguistic hypotheses that could help in the automatic identification of chain-starting definites. The information from different linguistic levels (lexical, semantic, morphological, syntactic, and pragmatic) in a computationally not expensive way casts light on potential features helpful for resolving coreference links. Each resulting heuristic managed to improve precision although at the cost of a drop in recall. The highest improvement in precision (89.20%) with the lowest loss in recall (78.22%) translates into an F 0.5 -measure of 85.21%. Hence, the incorporation of linguistic knowledge manages to outperform the baseline by 17 percentage points in precision. Priority is given to precision, since we want to assure that the filter prior to coreference resolution module does not label as chain starting definite NPs that are coreferent. The classifier was thus designed to minimize false positives. No less than 73% of definite NPs in the data set are chain starting, so detecting 78% of these definites with almost 90% precision could have substantial savings. From a linguistic perspective, the improvement in precision supports the linguistic hypotheses, even if at the expense of recall. However, as this classifier is not a final but a prior module, either a filter within a rule-based system or one additional feature within a larger learning-based system, the shortage of recall can be compensated at the coreference resolution stage by considering other more sophisticated features.The results here presented are not comparable with other existing classifiers of this type for several reasons. Our approach would perform differently for English, which has a lower number of definite NPs. Secondly, our classifier has been evaluated on a corpus much larger than prior ones such as. Thirdly, some classifiers aim at detecting non-anaphoric NPs, which are not the same as chain-starting. Fourthly, we have empirically explored the contribution of the set of heuristics with respect to the head_match feature. None of the existing approaches compares its final performance in relation with this simple but extremely powerful feature. Some of our heuristics do draw on previous work, but we have tuned them for Spanish and we have also contributed with new ideas, such as the use of storage units and the preference of some nouns fora specific syntactic type of modifier.As future work, we will adapt this chain-starting classifier for Catalan, fine-tune the set of heuristics, and explore to what extent the inclusion of such a classifier improves the overall performance of a coreference resolution system for Spanish. Alternatively, we will consider using the suggested attributes as part of a larger set of learning features for coreference resolution.",
,
Title 1: Automatic Detection of Noun Phrases Introducing Entities in Spanish,2
Title 2: A Corpus-Driven Classifier of Chain-Started Definite Noun Phrases in Spanish,
,
,
ID: 1589,
Abstract:,
"Semantic knowledge has been adopted recently for SMT preprocessing, decoding and evaluation, in order to be able to compare sentences based on their meaning rather than on mere lexical and syntactic similarity. Little attention has been paid to semantic knowledge in the context of integrating fuzzy matches from a translation memory with SMT. We present work in progress which focuses on semantics-based pretranslation before decoding in SMT. This involves applying fuzzy matching metrics based on lexical semantics and semantic roles, aligning parse trees based on semantic roles, and pretranslating matching source sentence parts using aligned tree nodes.",
Introduction:,
"Semantic knowledge has been adopted recently for SMT preprocessing, decoding and evaluation. Using such knowledge helps for comparing sentences based on meaning rather than form, and for moving away from the assumption of lexical and syntactic similarity between source and target sentences. Little attention has been paid to semantic knowledge in the context of integrating fuzzy matches with SMT. Fuzzy matching methods were originally designed for translation memories, in which translators store their translations. They are now also being used in the context of SMT, for pretranslating parts of sentences before or during decoding. These methods pretranslate matching sentence parts through word alignment, parse node alignment and phrase tables, and use different degrees of linguistic knowledge.As far as we know, semantic knowledge has not yet been applied for pretranslating sentence parts before decoding in SMT. Therefore, we would like to present our work in progress, which investigates, on the one hand, the use of semantic knowledge (lexical semantics and semantic roles) for improving the usability of fuzzy matches, and, on the other hand, the pretranslation of matching sentence parts using parse nodes aligned through semantic role information.In Section 2, we provide background on fuzzy matching and on semantic knowledge in SMT, including our own previous research on fuzzy matching and tree alignment. In Section 3, we provide the methodology we are currently devising for semantics-based pretranslation. As this is work in progress, results are not yet provided. However, the discussion of our recent work on combination of fuzzy matching metrics and on semantics-based tree alignment will hint at the potential of using additional sources of linguistic information, such as lexical semantics and semantic roles, for fuzzy matching.",
Conclusion:,
"We have shown the main ideas of GF and how they can be applied in NLP. The most mature applications are controlled-language tasks such as dissemination translation, language teaching, and natural language queries. Such task have commercial potential, and grammars gives full control on quality. GF makes the use of grammars fea-sible with its engineering tools and its library of 30 languages. The abstract structures originally created for European languages have proven to work for Chinese as well. GF also scales up to wide-coverage translation, but is not yet competitive with statistical methods. The main advantage in this task is the compact size of the system, making it possible to use 182 language pairs off-line in a mobile device.",
,
Title 1: Semantics-based Pretranslation for Fuzzy Matching in SMT,2
Title 2: Semantics-based Pretranslation before Decoding in SMT,
,
,
ID: 1862,
Abstract:,
"Developing Video-Grounded Dialogue Systems (VGDS), where a dialogue is conducted based on visual and audio aspects of a given video, is significantly more challenging than traditional image or text-grounded dialogue systems because (1) feature space of videos span across multiple picture frames, making it difficult to obtain semantic information; and (2) a dialogue agent must perceive and process information from different modalities (audio, video, caption, etc.) to obtain a comprehensive understanding. Most existing work is based on RNNs and sequence-to-sequence architec-tures, which are not very effective for capturing complex long-term dependencies (like in videos). To overcome this, we propose Mul-timodal Transformer Networks (MTN) to encode videos and incorporate information from different modalities. We also propose query-aware attention through an auto-encoder to extract query-aware features from non-text modalities. We develop a training procedure to simulate token-level decoding to improve the quality of generated responses during inference. We get state of the art performance on Dialogue System Technology Challenge 7 (DSTC7). Our model also generalizes to another multimodal visual-grounded dialogue task, and obtains promising performance.",
Introduction:,
"A video-grounded dialogue system (VGDS) generates appropriate conversational response to queries of humans, by not only keeping track of the relevant dialogue context, but also understanding the relevance of the query in the context of a given video (knowledge grounded in a video) ). An example dialogue exchange can be seen in. Developing such systems has recently received interest from the research community (e.g. DSTC7 challenge (). This task is much C: a man is standing in a kitchen putting groceries away. He closes the cabinet when finished, walks over to a table and pulls out a chair and sits down. S: a man puts away his groceries and then sits at a kitchen table and stares out the window. Q1: how many people are in the video? A1: there is just one person Q2: is there sound to the video? A2: yes there is audio but no one is talking ... Q10: is he happy or sad? A10: he appears to be neutral in expression: A sample dialogue from the DSTC7 Video Scene-aware Dialogue training set with 4 example video scenes. C: Video Caption, S: Video Summary, Qi: i th -turn question, Ai: i th -turn answer more challenging than traditional text-grounded or image-grounded dialogue systems because: (1) feature space of videos is larger and more complex than text-based or image-based features because of diverse information, such as background noise, human speech, flow of actions, etc. across multiple video frames; and (2) a conversational agent must have the ability to perceive and comprehend information from different modalities (text from dialogue history and human queries, visual and audio features from the video) and semantically shape a meaningful response to humans.Most existing approaches for multi-modal dialogue systems are based on RNNs as the sequence processing unit and sequence-to-sequence network as the overall architecture to model the sequential information in text (. Some efforts adopted query-aware attention to allow the models to focus on specific parts of the features most relevant to the dialogue context (. Despite promising results, these methods are not very effective or efficient for processing video-frames, due to the complexity of long term sequential information from multiple modalities. We propose Multimodal Transformer Networks (MTN) which model the complex sequential information from video frames, and also incorporate information from different modalities. MTNs allow for complex reasoning over multimodal data such as in videos, by jointly attending to information in different representation subspaces, and making it easier (than RNNs) to fuse information from different modalities. Inspired by the success of Transformers () for text, we propose novel neural architectures for VGDS: (1) We propose to capture complex sequential information from video frames using multi-head attention layers. Multihead attention is applied across several modalities (visual, audio, captions) repeatedly. This works like a memory network to allow the models to comprehensively reason over the video to answer human queries; (2) We propose an autoencoder component, designed as query-aware attention layer, to further improve the reasoning capability of the models on the non-text features of the input videos; and (3) We employ a training approach to improve the generated responses by simulating token-level decoding during training.We evaluated MTN on a video-grounded dialogue dataset (released through DSTC7 (). In each dialogue, video features such as audio, visual, and video caption, are available, which have to be processed and understood to hold a conversation. We conduct comprehensive experiments to validate our approach, including automatic evaluations, ablations, and qualitative analysis of our results. We also validate our approach on the visual-grounded dialogue task (, and show that MTN can generalize to other multimodal dialog systems.",
Conclusion:,
"In this paper, we showed that MTN, a multi-head attention-based neural network, can generate good conversational responses in multimodal settings. Our MTN models outperform the reported baseline and other submission entries to the DSTC7. We also adapted our approach to a visual dialogue task and achieved excellent performance. A possible improvement to our work is adding pre-trained embedding such as BERT ( or image-grounded word embedding ( to improve the semantic understanding capability of the models.",
,
Title 1: Video-Grounded Dialogue Systems with Multimodal Transformers,
Title 2: Video-Grounded Dialogue Systems with Multimodal Transformers,0
,
,
ID: 1947,
Abstract:,
"Being able to induce word translations from non-parallel data is often a prerequisite for cross-lingual processing in resource-scarce languages and domains. Previous endeavors typically simplify this task by imposing the one-to-one translation assumption, which is too strong to hold for natural languages. We remove this constraint by introducing the Earth Mover's Distance into the training of bilingual word embeddings. In this way, we take advantage of its capability to handle multiple alternative word translations in a natural form of regularization. Our approach shows significant and consistent improvements across four language pairs. We also demonstrate that our approach is particularly preferable in resource-scarce settings as it only requires a minimal seed lexicon.",
Introduction:,
"Bilingual lexica provide word-level semantic equivalence information across languages, and prove to be valuable fora range of cross-lingual natural language processing tasks. As building bilingual lexica from parallel corpora has been solved byword alignment, researchers have turned their attention to non-parallel corpora. Accompanied by a small seed lexicon, non-parallel corpora are usually the only resources available in resource-scarce languages and domains, making the task of bilingual lexicon induction both important and challenging. A variety of statistical methods have been proposed to induce bilingual lexica from non-parallel data. With the surge of word embeddings trained by neural networks, recent approaches that learn bilingual word representations from non-parallel data for bilingual lexicon induction have also shown promise (.However, none of the existing methods explicitly considers multiple alternative translation, i.e., the phenomenon that one source language word may have multiple possible translations in the target language. For example, the (romanized) Chinese word ""qiche"" can be translated to ""car"" or ""automobile"" in English, while the English word ""car"" can mean ""qiche"" or ""chexiang"" (railway carriage) in Chinese. Although prevalent among natural languages), multiple alternative translation is basically ignored by prior bilingual lexicon inducers; instead, they typically impose the one-to-one translation assumption) for simplicity. This represents a major drawback of existing bilingual lexicon induction approaches.There has been one study that shows potential for tackling this issue. It introduces the Earth Mover's Distance (EMD) (. Given learned bilingual word embeddings, the EMD is used as a post-processing step to match vocabularies cross-lingually, which can be interpreted as word translation. Unlike the traditional K nearest neighbors leaving the determination of the number of translation proposals K to the user, the EMD automatically determines the list of translation candidates for each source word. In this work, we propose to bring the EMD's capability to training. Intuitively, as the EMD in the post-processing step is able to connect a source word with multiple target word translations, it can play a more important role during training by driving the word vectors of these mutual translations to be closer. We therefore expect that the bilingual word embeddings learned this way will be more suitable for encoding multiple alternative translation by harnessing the power of the EMD. Our experiments validate the effectiveness of this strategy. A summary of our contributions is as follows:• We introduce the Earth Mover's Distance into the training of bilingual word embeddings, and interpret it as a natural form of regularization for the overall learning objective (Section 3).• We demonstrate significant and consistent performance improvement from our strategy across four language pairs (Sections 6.1 and 6.2).• We investigate the effect of the number of seed word translation pairs, and find our approach to be most appealing with few seeds, inline with typical resource-scarce scenarios (Section 6.3).",
Conclusion:,
"In this paper, we look into multiple alternative translations prevalent across natural languages, which are largely neglected in previous bilingual lexicon induction research. We propose to introduce the Earth Mover's Distance into the training of bilingual word embeddings as a natural form of regularization. We provide strong empirical results for four language pairs to demonstrate the effectiveness of our approach. Furthermore, we discover that our method remains reliable with rather few seed word translation pairs, unlike the baselines exhibiting performance degradation. This advantage of our approach is particularly desirable in realistic resource-scarce settings.",
,
Title 1: Bilingual Word Embeddings with Earth Mover's Distance for Cross-lingual Translation,
Title 2: Bilingual Lexicon Induction with the Earth Mover's Distance,2
,
,
ID: 803,
Abstract:,
"Multilingual writers and speakers often alternate between two languages in a single discourse , a practice called ""code-switching"". Existing sentiment detection methods are usually trained on sentiment-labeled monolingual text. Manually labeled code-switched text, especially involving minority languages, is extremely rare. Consequently, the best mono-lingual methods perform relatively poorly on code-switched text. We present an effective technique for synthesizing labeled code-switched text from labeled monolingual text, which is more readily available. The idea is to replace carefully selected subtrees of constituency parses of sentences in the resource-rich language with suitable token spans selected from automatic translations to the resource-poor language. By augmenting scarce human-labeled code-switched text with plentiful synthetic code-switched text, we achieve significant improvements in sentiment labeling accuracy (1.5%, 5.11%, 7.20%) for three different language pairs (English-Hindi, English-Spanish and English-Bengali). We also get significant gains for hate speech detection: 4% improvement using only synthetic text and 6% if augmented with real text.",
Introduction:,
"Sentiment analysis on social media is critical for commerce and governance. Multilingual social media users often use code-switching, particularly to express emotion ( . However, a basic requirement to train any sentiment analysis (SA) system is the availability of large sentimentlabeled corpora. These are extremely challenging to obtain), requiring volunteers fluent in multiple languages.We present CSGen, a system which provides supervised SA algorithms with synthesized unlimited sentiment-tagged code-switched text, without involving human labelers of code-switched text, or any linguistic theory or grammar for codeswitching. These texts can then train state-ofthe-art SA algorithms which, until now, primarily worked with monolingual text.A common scenario in code-switching is that a resource-rich source language is mixed with a resource-poor target language. Given a sentimentlabeled source corpus, we first create a parallel corpus by translating to the target language, using a standard translator. Although existing neural machine translators (NMTs) can translate a complete source sentence to a target sentence with good quality, it is difficult to translate only designated source segments in isolation because of missing context and lack of coherent semantics.Among our key contributions is a suite of approaches to automatic segment conversion. Broadly, given a source segment selected for codeswitching, we propose intuitive ways to select a corresponding segment from the target sentence, based on maximum similarity or minimum dissimilarity with the source segment, so that the segment blends naturally in the outer source context. Finally, the generated synthetic sentence is tagged with the same sentiment label as the source sentence. The source segment to replace is carefully chosen based on an observation that, apart from natural switching points dictated by syntax, there is a propensity to code-switch between highly opinionated segments.Extensive experiments show that augmenting scarce natural labeled code-switched text with plentiful synthetic text associated with 'borrowed' source labels enriches the feature space, enhances its coverage, and improves sentiment detection accuracy, compared to using only natural text. On four natural corpora having gold sentiment tags, we demonstrate that adding synthetic text can improve accuracy by 5.11% in English-Spanish, 7.20% in English-Bengali and (1.5%, 0.97%) in English-Hindi. The synthetic code-switch text, even when used by itself to train SA, performs almost as well as natural text in several cases. Hate speech is an extreme emotion expressed often on social media. On an EnglishHindi gold-tagged hate speech benchmark, we achieve 6% absolute F1 improvement with data augmentation, partly because synthetic text mitigates label imbalance present in scarce real text.",
Conclusion:,
"Code-mixing is an important and rapidly evolving mechanism of expression among multilingual populations on social media. Monolingual sentiment analysis techniques perform poorly on codemixed text, partly because code-mixed text often involves resource-poor languages. Starting from sentiment-labeled text in resource-rich source languages, we propose an effective method to synthesize labeled code-mixed text without designing switching grammars. Augmenting scarce natural text with synthetic text improves sentiment detection accuracy.",
,
Title 1: CSGen: Synthetic Code-Switched Text for Sentiment Analysis,
Title 2: Improving Sentiment Labeling on Manually Labeled Code-Switched Text,2
,
,
ID: 918,
Abstract:,
"We describe the semi-automatic adaptation of a TimeML annotated corpus from English to Portuguese, a language for which TimeML annotated data was not available yet. In order to validate this adaptation, we use the obtained data to replicate some results in the literature that used the original English data. The fact that comparable results are obtained indicates that our approach can be used successfully to rapidly create semantically annotated resources for new languages.",
Introduction:,
"Temporal information processing is a topic of natural language processing boosted by recent evaluation campaigns like TERN2004, 1 TempEval-1 () and the forthcoming TempEval-2 2. Even when the best performing systems in these competitions are symbolic, there are machine learning solutions with results close to their performance. In TempEval-1, where there were statistical and rule-based systems, almost all systems achieved quite similar results. In the TERN2004 competition (aimed at identifying and normalizing temporal expressions), a symbolic system performed best, but since then machine learning solutions, such as (, have appeared that obtain similar results.These evaluations made available sets of annotated data for English and other languages, used for training and evaluation. One natural question to ask is whether it is feasible to adapt the training and test data made available in these competitions to other languages, for which no such data still exist. Since the annotations are largely of a semantic nature, not many changes need to be done in the annotations once the textual material is translated. In essence, this would be a fast way to create temporal information processing systems for languages for which there are no annotated data yet.In this paper, we report on an experiment that consisted in adapting the English data of TempEval-1 to Portuguese. The results of machine learning algorithms over the data thus obtained are compared to those reported for the English TempEval-1 competition. There is of course the caveat that the adaptation process can introduce errors.This paper proceeds as follows. In Section 2, we provide a quick overview of the TimeML annotations in the TempEval-1 data. In Section 3, it is described how the data were adapted to Portuguese. Section 4 contains a brief quantitative comparison of the two corpora. In Section 5, the results of replicating one of the approaches present in the TempEval-1 challenge with the Portuguese data are presented. We conclude this paper in Section 6. contains an example of a document from the TempEval-1 corpus, which is similar to the TimeBank corpus (.",
Conclusion:,
"Current SRL approaches limit the search for arguments to the sentence containing the predicate of interest. Many systems take this assumption a step further and restrict the search to the predicate's local syntactic environment; however, predicates and the sentences that contain them rarely exist in isolation. As shown throughout this paper, they are usually embedded in a coherent and semantically rich discourse that must betaken into account. We have presented a preliminary study of implicit arguments for nominal predicates that focused specifically on this problem. First, we have created gold-standard implicit argument annotations fora small set of pervasive nominal predicates. Our analysis shows that these annotations add 65% to the role coverage of NomBank. Second, we have demonstrated the feasibility of recovering implicit arguments for many of the predicates, thus establishing a baseline for future work on this emerging task. Third, our study suggests a few ways in which this research can be moved forward. As shown in Section 6, many errors were caused by the absence of true implicit arguments within the set of candidate constituents. More intelligent windowing strategies in addition to alternate candidate sources might offer some improvement. Although we consistently observed development gains from using automatic coreference resolution, this process creates errors that need to be studied more closely. It will also be important to study implicit argument patterns of non-verbal predicates such as the partitive percent. These predicates are among the most frequent in the TreeBank and are likely to require approaches that differ from the ones we pursued.Finally, any extension of this work is likely to encounter a significant knowledge acquisition bottleneck. Implicit argument annotation is difficult because it requires both argument and coreference identification (the data produced by is similar). Thus, it might be productive to focus future work on (1) the extraction of relevant knowledge from existing resources (e.g., our use of coreference patterns from Gigaword) or (2) semi-supervised learning of implicit argument models from a combination of labeled and unlabeled data.",
,
Title 1: Adapting a TimeML annotated corpus from English to Portuguese,1
Title 2: Adapting a TimeML Annotated Corpus to Portuguese,
,
,
ID: 894,
Abstract:,
"This paper describes speech translation from Amharic-to-English, particularly Automatic Speech Recognition (ASR) with post-editing feature and Amharic-English Statistical Machine Translation (SMT). ASR experiment is conducted using morpheme language model (LM) and phoneme acoustic model (AM). Likewise, SMT conducted using word and morpheme as unit. Morpheme based translation shows a 6.29 BLEU score at a 76.4% of recognition accuracy while word based translation shows a 12.83 BLEU score using 77.4% word recognition accuracy. Further, after post-edit on Amharic ASR using corpus based n-gram, the word recognition accuracy increased by 1.42%. Since post-edit approach reduces error propagation, the word based translation accuracy improved by 0.25 (1.95%) BLEU score. We are now working towards further improving propagated errors through different algorithms at each unit of speech translation cascading component.",
Introduction:,
"Speech is one of the most natural form of communication for humankind. Computer with the ability to understand natural language promoted the development of man-machine interface. This can be extended through different digital platforms such as radio, mobile, TV, CD and others. Through these, speech translation facilitates communication between the people who speak different languages.Speech translation is the process by which spoken source phrases are translated to a target language using a computer (). Speech translation research for major and technological supported languages like English, European languages (like French and Spanish) and Asian languages (like Japanese and Chinese) has been conducted since the 1983s by NEC Corporation. The advancement of speech translation captivates the communication between people who do not share the same language.The state-of-the-art of speech translation system can be seen as the integration of three major cascading components (; Automatic Speech Recognition (ASR), Machine Translation (MT) and Text-ToSpeech (TTS) synthesis. ASR is the process by which a machine infers spoken words, by means of talking to computer, and having it correctly understand a recorded audio signal. Beside ASR, MT is the process by which a machine is used to translate a text from one source language to another target language. Finally, TTS creates a spoken version from the text of electronic document such as text file and web document.As one major component of speech translation, Amharic ASR started in. A number of attempts have been made for Amharic ASR using different methods and techniques towards designing speaker independent, large vocabulary, contineous speech and spontaneous speech recognition.In addition to ASR, a preliminary EnglishAmharic machine translation experiments was conducted using phonemic transcription on the Amharic corpus (). The result obtained from the experiment shows that, it is possible to design English-Amharic machine translation using statistical method.As the last component of speech translation, a number of TTS research have been attempted using different techniques and methods as discussed by). Among these, concatenative, cepstral, formant and a syllable based speech synthesizers were the main methods and techniques applied.All the above research works were conducted using different methods and techniques beside data difference and integration as a cascading component. Moreover, dataset and tools used in the above research are not accessible which makes difficult to evaluate the advancement of research in speech technology for local languages.However, there is no attempt to integrate ASR, SMT and TTS to come up with speech translation system for Amharic language. Thus, the main aim of this study is to investigate the possibility to design Amharic-English speech translation system that controls recognition errors propagating through cascading components.",
Conclusion:,
"We have presented the methods, the data, the evaluation setup, and the results for four shared tasks taht we organized as part of the VarDial 2017 evaluation campaign. To the best of our knowledge, this is the first comprehensive evaluation campaign on NLP for Similar Languages, Varieties and Dialects. Three tasks (ADI, GDI, and DSL) dealt with dialect and language variety identification, focusing on Arabic, German and several groups of similar languages, respectively, whereas the CLP task dealt with parsing.Along with the results of each shared task, we also included short descriptions of each participating system in order to provide readers with an overview of all approaches proposed for each task. For a complete description of each system, we included references to the fifteen system description papers that were accepted for presentation at the VarDial workshop at EACL'2017.Given the success of the VarDial evaluation campaign, we believe that there is room for another edition with more shared tasks. Possible topics of interest for future shared tasks include machine translation between similar languages and POS tagging of dialects, among others.",
,
Title 1: Automatic Speech Recognition and Statistical Machine Translation for Amharic Language,1
Title 2: Automatic Speech Translation from Amharic-English using Post-Editing Feature and Statistical Machine Translation,
,
,
ID: 1080,
Abstract:,
"We use machine learners trained on a combination of acoustic confidence and pragmatic plausi-bility features computed from dialogue context to predict the accuracy of incoming n-best recognition hypotheses to a spoken dialogue system. Our best results show a 25% weighted f-score improvement over a baseline system that implements a ""grammar-switching"" approach to context-sensitive speech recognition.",
Introduction:,
"A crucial problem in the design of spoken dialogue systems is to decide for incoming recognition hypotheses whether a system should accept (consider correctly recognized), reject (assume misrecognition), or ignore (classify as noise or speech not directed to the system) them. In addition, a more sophisticated dialogue system might decide whether to clarify or confirm certain hypotheses.Obviously, incorrect decisions at this point can have serious negative effects on system usability and user satisfaction. On the one hand, accepting misrecognized hypotheses leads to misunderstandings and unintended system behaviors which are usually difficult to recover from. On the other hand, users might get frustrated with a system that behaves too cautiously and rejects or ignores too many utterances. Thus an important feature in dialogue system engineering is the tradeoff between avoiding task failure (due to misrecognitions) and promoting overall dialogue efficiency, flow, and naturalness.In this paper, we investigate the use of machine learners trained on a combination of acoustic confidence and pragmatic plausibility features (i.e. computed from dialogue context) to predict the quality of incoming n-best recognition hypotheses to a spoken dialogue system. These predictions are then used to select a ""best"" hypothesis and to decide on appropriate system reactions. We evaluate this approach in comparison with a baseline system that combines fixed recognition confidence rejection thresholds with dialogue-state dependent recognition grammars.The paper is organized as follows. After a short relation to previous work, Section 3 introduces the WITAS multimodal dialogue system, which we use to collect data (Section 4) and to derive baseline results (Section 5). Section 6 describes our learning experiments for classifying and selecting from nbest recognition hypotheses and Section 7 reports our results.",
Conclusion:,
"We used a combination of acoustic confidence and pragmatic plausibility features (i.e. computed from dialogue context) to predict the quality of incoming recognition hypotheses to a multi-modal dialogue system. We classified hypotheses as accept, (clarify), reject, or ignore: functional categories that can be used by a dialogue manager to decide appropriate system reactions. The approach is novel in combining machine learning with n-best processing for spoken dialogue systems using the Information State Update approach.Our best results, obtained using TiMBL with optimized parameters, show a 25% weighted f-score improvement over a baseline system that uses a ""grammar-switching"" approach to context-sensitive speech recognition, and are only 8% away from the optimal performance that can be achieved on the data. Clearly, this improvement would result in better dialogue system performance overall. Parameter optimization improved the classification results by 9% compared to using the learner with default settings, which shows the importance of such tuning.Future work points in two directions: first, integrating our methodology into working ISU-based dialogue systems and determining whether or not they improve in terms of standard dialogue evaluation metrics (e.g. task completion). The ISU approach is a particularly useful testbed for our methodology because it collects information pertaining to dialogue context in a central data structure from which it can be easily extracted. This avenue will be further explored in the TALK project 8 . Second, it will be interesting to investigate the impact of different dialogue and task features for classification and to introduce a distinction between ""generic"" features that are domain independent and ""application-specific"" features which reflect properties of individual systems and application scenarios.",
,
Title 1: Using Acoustic Confidence and Pragmatic Plausibility to Classify N-Best Recognition Hypotheses to a Multi-Modal Dialogue System,
Title 2: Context-Sensitive Speech Recognition in Spoken Dialogue Systems,2
,
,
ID: 299,
Abstract:,
"As the volume of documents on the Web increases , technologies to extract useful information from them become increasingly essential. For instance, information extracted from social network services such as Twitter and Facebook is useful because it contains a lot of location-specific information. To extract such information, it is necessary to identify the location of each location-relevant expression within a document. Previous studies on location disambiguation have tackled this problem on the basis of word sense disambigua-tion, and did not make use of location-specific clues. In this paper, we propose a method for location disambiguation that takes advantage of the following two clues: spatial proximity and temporal consistency. We confirm the effectiveness of these clues through experiments on Twitter tweets with GPS information.",
Introduction:,
"As the volume of documents on the Web increases, technologies to extract useful information from them become increasingly essential. For instance, information extracted from social network services (SNS) such as Twitter and Facebook is useful because it contains a lot of location-specific information. To extract such information, it is necessary to identify the location of each location-relevant expression within a document.However, many previous studies on SNS rely only on geo-tagged documents (e.g.,), which include GPS information, but these represent only a small proportion of the total. To extract as much location information as possible, it is important to develop a method that can estimate locations from numerous documents without GPS information.Previous studies on location disambiguation made use of methods for word sense disambiguation and are based only on textual information, i.e., the bagof-words in a document. It is, however, difficult to solve this problem using only textual information in a relatively short SNS document. For example, it is difficult to identify the location of ""Prefectural Office Ave."" from the following document based only on word information. ""I arrived at Prefectural Office Ave. from Shuri Station!""In this paper, we propose a method that identifies the locations of location expressions in Twitter tweets on the basis of the following two clues: (1) spatial proximity, and (2) temporal consistency. Spatial proximity assumes that all locations mentioned in a tweet are close to one another. In the above document, for example, we would assume that ""Prefectural Office Ave."" is ""Prefectural Office Ave. (Okinawa)"" using the proximity between ""Shuri Station"" and ""Prefectural Office Ave. (Okinawa)"" The other clue is temporal consistency, Semiocast reported that GPS information is assigned to only 0.77% of all public tweets. Although it is possible to learn a clue from ""Shuri Station,"" which is located in Okinawa Prefecture, it would require a large amount of training data to learn such lexical clues for each target location expression.1 which assumes that the locations in a series of tweets are near to each other.In our experiments, we learn a location classifier for each ambiguous location expression in Japanese. Hereafter, we call an ambiguous location expression, such as ""Prefectural Office Ave.,"" a Location EXpression (LEX), and a location to which a LEX points, such as <Prefectural Office Ave. (Okinawa)>, a Location Entity (LE), which is linked to its GIS information. We calla LEX linked to multiple LEs an ambiguous LEX, which is the target of our location name disambiguation system. That is unambiguous LEXs are not our target, such as ""Tokyo Tower,"" which points the LE <Tokyo Tower>.We define a set of LEXs and LEs on the basis of Japanese Wikipedia. Training data for the location classifiers are created from tweets containing GPS information. The resulting location classifiers can be applied to LEXs in any tweets or documents without GPS information.Our novel contributions can be summarized as follows:• two novel clues for location disambiguation are proposed, • training data is automatically created from tweets with GPS information, and • our method can identify LEs of LEXs in any documents without GPS information.The remainder of this paper is organized as follows. Section 2 introduces related work, while Section 3 describes the resources used in this paper. Section 4 details our proposed method and Section 5 reports the experimental results. Section 6 concludes the paper.",
Conclusion:,
"In this paper, we presented a method for location name disambiguation for text snippets on SNS. We considered both the spatial proximity and temporal consistency to produce the estimates of LEs. As a result, our method substantially outperformed the baseline method that considers only lexical information. More specifically:• Considering the spatial proximity improves the accuracy• Considering the temporal consistency with many tweets improves the accuracy• Considering both of the above outperforms the baseline by 7.13%In future work, first, we plan to further investigate the cause of the decrease inaccuracy when the temporal consistency feature considers many tweets.Second, in this paper, only tweets including unambiguous LEXs are used to calculate the proximity feature for the target LEX. However, tweets including ambiguous LEXs could also be used if the LEXs have been disambiguated in advance.In addition, we estimated the LEs of ambiguous LEXs, although the location estimation has several problems. One concerns whether the user posting the tweet including the LEX is actually at that location. Solving this problem is necessary for some applications specializing in GIS information. In future work, we aim to solve this problem using the proposed spatial proximity and temporal consistency.",
,
Title 1: Location Disambiguation Using Spatial Clustering and Temporal Consistency,
Title 2: Location Disambiguation Using Spatial and Temporal Clues,2
,
,
ID: 1493,
Abstract:,
"We present a holistic data-driven approach to image description generation, exploiting the vast amount of (noisy) parallel image data and associated natural language descriptions available on the web. More specifically, given a query image, we retrieve existing human-composed phrases used to describe visually similar images, then selectively combine those phrases to generate a novel description for the query image. We cast the generation process as constraint optimization problems, collectively incorporating multiple interconnected aspects of language composition for content planning, surface realization and discourse structure. Evaluation by human annotators indicates that our final system generates more semantically correct and linguistically appealing descriptions than two nontrivial baselines.",
Introduction:,
"Automatically describing images in natural language is an intriguing, but complex AI task, requiring accurate computational visual recognition, comprehensive world knowledge, and natural language generation. Some past research has simplified the general image description goal by assuming that relevant text for an image is provided (e.g.,, ). This allows descriptions to be generated using effective summarization techniques with relatively surface level image understanding. However, such text (e.g., news articles or encyclopedic text) is often only loosely related to an image's specific content and many natural images do not come with associated text for summarization.In contrast, other recent work has focused more on the visual recognition aspect by detecting content elements (e.g., scenes, objects, attributes, actions, etc) and then composing descriptions from scratch (e.g.,, ,, ), or by retrieving existing whole descriptions from visually similar images (e.g.,,). For the latter approaches, it is unrealistic to expect that there will always exist a single complete description for retrieval that is pertinent to a given query image. For the former approaches, visual recognition first generates an intermediate representation of image content using a set of English words, then language generation constructs a full description by adding function words and optionally applying simple re-ordering. Because the generation process sticks relatively closely to the recognized content, the resulting descriptions often lack the kind of coverage, creativity, and complexity typically found in humanwritten text.In this paper, we propose a holistic datadriven approach that combines and extends the best aspects of these previous approaches -a) using visual recognition to directly predict individual image content elements, and b) using retrieval from existing human-composed descriptions to generate natural, creative, and inter-esting captions. We also lift the restriction of retrieving existing whole descriptions by gathering visually relevant phrases which we combine to produce novel and query-image specific descriptions. By judiciously exploiting the correspondence between image content elements and phrases, it is possible to generate natural language descriptions that are substantially richer in content and more linguistically interesting than previous work.At a high level, our approach can be motivated by linguistic theories about the connection between reading activities and writing skills, i.e., substantial reading enriches writing skills, (e.g.,,). Analogously, our generation algorithm attains a higher level of linguistic sophistication by reading large amounts of descriptive text available online. Our approach is also motivated by language grounding by visual worlds (e.g.,,,), as in our approach the meaning of a phrase in a description is implicitly grounded by the relevant content of the image.Another important thrust of this work is collective image-level content-planning, integrating saliency, content relations, and discourse structure based on statistics drawn from a large image-text parallel corpus. This contrasts with previous approaches that generate multiple sentences without considering discourse flow or redundancy (e.g., ). For example, for an image showing a flock of birds, generating a large number of sentences stating the relative position of each bird is probably not useful.Content planning and phrase synthesis can be naturally viewed as constraint optimization problems. We employ Integer Linear Programming (ILP) as an optimization framework that has been used successfully in other generation tasks (e.g.,,, ). Our ILP formulation encodes a rich set of linguistically motivated constraints and weights that incorporate multiple aspects of the generation process. Empirical results demonstrate that our final system generates linguistically more appealing and semantically more correct descriptions than two nontrivial baselines.",
Conclusion:,
The results described in this paper show that the LetsMT! project is on track to fulfill its goal to democratise the creation and usage of custom SMT systems. demonstrates that the open source SMT toolkit Moses is reaching maturity to serve as abase for large scale and heavy use production purposes. The architecture of the platform and Resource Repository enables scalability of the system and very large amounts of data to be handled in a variety of formats. Evaluation shows a strong increase in translation productivity by using LetsMT!,
,
Title 1: A Holistic Data-Driven Approach to Image Description Generation,0
Title 2: A Holistic Data-Driven Approach to Image Description Generation,
,
,
ID: 965,
Abstract:,
"Originating from a multidisciplinary research project that gathers,",
Introduction:,
"In parallel with the Semantic Web's extremely active research community lies a continuous and exceptionally rising propagation of the Social Web. A remarkable advancement can be made if a proper methodology for maximizing the cooperation between the two webs can beset. Such a methodology should highly encourage the first Web to bring in its theories and formalisms to the second, in exchange for some of the latter's popularity and proliferation.An amplified fusion between the Social and the Semantic Webs is indeed a strongly beneficial achievement to both disciplines. It shall solve the foremost problems undergone by each of them, yielding an outcome that by far surpasses the sum of its individual components by endorsing automation, standardization and interoperability, promoting efficient information extraction, querying and aggregations, and providing valuable large data sets to feed the Semantic Web applications from the abundant social networking Web 2.0 sites (SNS). These sites will successively benefit from Semantic Web applications to generate semantically-rich data, and an overall reflection of the henceforth strongly formalized Social Web's network effect on the Semantic Web, boosting its formerly limited usage (.By delving into the Semantic Web's main achievements for Social Networking (SN), this research notices alack in those involving the Semantic Web's advanced findings and relatively complicated vocabularies and grammars, particularly in the endeavors related to OWL 2 (Web Ontology Language) novelties. In addition, it recognizes the major limitations and concerns related to complexity and accuracy when dealing with ontology-aware Natural Language Processing for large amounts of data.As a consequence, it proposes a promising flexible and multidimensional user and NLPassisted workflow for social data management encompassing different strategies varying according to prerequisite constraints and concerns. On the other hand, it confers a Web 2.0 user collaboration novelty residing in promoting SN users ""rule tagging"" assignments that are initiated on account of domain-specific semantic arrangements in the knowledge base repository. Section 3 presents a very brief overview of the enclosing knowledge framework and platform; in Section 4, the user and NLP-assisted workflow is portrayed and analyzed; Section 5 exposes OWL 2-supported demonstrating scenarios that endow with recommender systems based on an ontology for childhood obesity surveillance. We finally wrap up with a conclusions section that comprises a closing discussion and highlights on future work.",
Conclusion:,
"Apart from providing a maximal set of consistent and accurate semantics, fostering such a user and NLP-assisted workflow can prove to be advantageous at many levels. We can underline a few extra issues, by considering for example the ""Open World Assumption"" which is evidently appropriate for the context of textual blog information dealt within this research: a statement or fact not explicitly mentioned in a blog does not disprove its existence. Nevertheless, to deal with certain critical rules and axioms, for which the availability of accurate data is deemed much more valuable for our working framework, an exclusive approach can be embraced in order to possibly ""close the world"" related to these critical facts. Closing axioms can be identified in our backbone repository, and presented to the user, inviting them to key in their exact input. Furthermore, the intensional reasoning required in any application involving natural language processing presents DL-safety restrictions, due to conclusions referring to unnamed objects. By offering this user rule tagging facility, we can limit the effects of such constraints. In all cases, relying on a collective effort through which rules and semantics are gathered and validated, before becoming instance and ontology enrichment elements is a much more profitable and effective approach.A well-populated knowledge base, henceforth enriched with semantically engineered social data, is consequently accessible for further extensive reasoning and analysis. The outcome reached surpasses by far the sum of its social and semantic data components, typically leading to significant services and recommender systems.Taking into consideration the applicable involved reasoning, the opportunity of identifying, creating and expanding social and semantic networks is presented. Implemented algorithms allow opinion mining, detection of ties and similarities between people, leading to connections via shared interests or any possible common ground areas. For instance, semantic networks are initiated based on the algorithms' ability to retrieve people with same or similar goals, tastes, origins, backgrounds, etc., and to further apply advanced reasoning with the intention of providing suggestions, recommendations, possible solutions, feedbacks, openings, and soon.",
,
Title 1: A User and NLP-assisted Workflow for Social Data Management,
Title 2: A Report on the 7th Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Text,-1
,
,
ID: 1918,
Abstract:,
"This software demonstration presents WebLicht (short for: Web-Based Linguistic Chaining Tool), a web-based service environment for the integration and use of language resources and tools (LRT). WebLicht is being developed as part of the D-SPIN project 1. We-bLicht is implemented as a web application so that there is no need for users to install any software on their own computers or to concern themselves with the technical details involved in building tool chains. The integrated web services are part of a prototypical infrastructure that was developed to facilitate chaining of LRT services. WebLicht allows the integration and use of distributed web services with standardized APIs. The nature of these open and standardized APIs makes it possible to access the web services from nearly any programming language, shell script or workflow engine (UIMA, Gate etc.) Additionally, an application for integration of additional services is available, allowing anyone to contribute his own web service.",
Introduction:,
"Currently, WebLicht offers LRT services that were developed independently at the Institut für Informatik, Abteilung Automatische Sprachverarbeitung at the University of Leipzig (tokenizer, lemmatizer, co-occurrence extraction, and frequency analyzer), at the Institut für Maschinelle Sprachverarbeitung at the University of Stuttgart (tokenizer, tagger/lemmatizer, German morphological analyser SMOR, constituent and dependency parsers), at the Berlin Brandenburgische Akademie der Wissenschaften (conversion of plain text to D-Spin format, tokenizer, taggers, NE recog-",
Conclusion:,
"In this paper we have described the functionality of GernEdiT. The extremely positive feedback of the GermaNet lexicographers underscores the practical benefits gained by using the GernEdiT tool in practice.At the moment, GernEdiT is customized for maintaining the GermaNet data. In future work, we plan to adapt the tool so that it can be used with wordnets for other languages as well. This would mean that the wordnet data fora given language would have to be stored in a relational database and that the tool itself can handle the language specific data structures of the wordnet in question.Deutsche Rechtschreibung -Regeln und Wörterverzeichnis: Amtliche Regelung.",
,
Title 1: WebLicht: Web-Based Linguistic Chaining Tool,2
Title 2: WebLicht: A Web-based Linguistic Chaining Tool,
,
,
ID: 361,
Abstract:,
"The paper introduces anew annotation of discourse relations in the Prague Dependency Treebank (PDT), i.e. the annotation of the so called secondary connectives (mainly multiword phrases like the condition is, that is the reason why, to conclude, this means etc.). Firstly, the paper concentrates on theoretical introduction of these expressions (mainly with respect to primary connectives like and, but, or, too etc.) and tries to contribute to the description and definition of discourse connectives in general (both primary and secondary). Secondly, the paper demonstrates possibilities of annotations of secondary connectives in large corpora (like PDT). The paper describes general annotation principles for secondary connectives used in PDT for Czech and compares the results of this annotation with annotation of primary connectives in PDT. In this respect, the main aim of the paper is to introduce anew type of discourse annotation that could be adopted also by other languages.",
Introduction:,
"In the paper, we introduce anew annotation of discourse relations in the Prague Dependency Treebank (PDT) enriched by the so called secondary connectives (i.e. especially by the multiword phrases like hlavním důvodem je ""the main reason is"", závěr zní ""the conclusion is"", to kontrastuje s tím ""this contrasts with"" etc.).We present how it is possible to annotate such variable (i.e. inflectional and modifiable) structures on big data according to general annotation principles. We believe that our methods maybe used also for other languages to enrich the discourse annotations of similar corpora.",
Conclusion:,
"In the paper, we have introduced the annotation of the so called secondary connectives (i.e. expressions like the condition is, to conclude, for these reasons etc.).From theoretical point of view, we define discourse connectives as (mostly) universal indicators of discourse relations that may have different surface forms. According of their realization, we distinguish primary and secondary connectives. Primary connectives are expressions with universal status of discourse indicators that are grammaticalized (i.e. lexically frozen). They are functional words (i.e. mainly conjunctions and structuring particles) that are not integrated into clause structure as sentence elements like but, and, or, because etc. Secondary connectives are mainly multiword phrases containing a lexical word or words that are not yet fully grammaticalized; therefore, these structures are much more variable (concerning modification, inflexion etc.). The secondary connectives maybe sentence elements (because of this), sentence modifiers (simply speaking) or they form a separate sentence (The reason is simple.).In the paper, we demonstrated how it is possible to include secondary connectives into corpus annotations. The overall inter-annotator agreement on existence of a discourse relation is 0.70 (F1) and on the type of a discourse relation 0.82 (0.78 C. k.), which is very similar to primary connectives in PDT.Altogether, PDT contains 1,161 tokens of secondary connectives, which is 5.4 % within all explicit discourse connectives in PDT (thus the attribute secondary seems suitable for them also in terms of frequency).We have compared primary and secondary connectives also in terms of semantic types of discourse relations they express. The distribution of the individual semantic relations is very similar for both primary and secondary connectives (with some exceptions like the relation of opposition occurring very predominantly with primary connectives). However, the annotation has taught us that the classification of relations formulated for primary connectives cannot be simply adopted for secondary connectives -during the annotation, we have observed three ""new"" semantic types (that were not included into the classification for primary connectives): a) entailment or deduction of results (e.g. it follows); b) the relation of conclusion (e.g. the conclusion is); c) the relation of regard (e.g. in this respect). These three types of relation refer mostly to larger pieces of text like a whole paragraph.The results of annotation also demonstrate that primary and secondary connectives differ in terms of inter-and intra-sentential relations. Whereas primary connectives prefer the intrasentential relations (in 70 %), secondary connectives mostly the inter-sentential relations (in 63 %). So primary and secondary connectives do not differ only from syntactic, lexical and semantic point of view, but also in the way how they structure the text.At the current stage, the Prague Dependency Treebank contains the most detailed annotation of secondary connectives (as far as we know, done on the largest data) that could be adopted also for other languages in other corpora focusing mostly on the annotation of primary connectives. In the paper, we tried to demonstrate that discourse annotation including secondary connectives is more complete and that similar analysis may lead to better understanding of discourse.",
,
Title 1: Annotating Discourse Relations in the Prague Dependency Treebank,1
Title 2: Annotating secondary connectives in the Prague Dependency Treebank,
,
,
ID: 1535,
Abstract:,
"Lexical Simplification is the task of modifying the lexical content of complex sentences in order to make them simpler. Due to the lack of reliable resources available for the task, most existing approaches have difficulties producing simplifications which are grammatical and that preserve the meaning of the original text. In order to improve on the state-of-the-art of this task, we propose user studies with non-native speakers, which will result in new, sizeable datasets, as well as novel ways of performing Lexical Simplification. The results of our first experiments show that new types of classifiers, along with the use of additional resources such as spoken text language models , produce the state-of-the-art results for the Lexical Simplification task of SemEval-2012.",
Introduction:,
"Lexical Simplification (LS) is often perceived as the simplest of all Text Simplification sub-tasks. Its goal is to replace the complex words and expressions of a given sentence with simpler alternatives of equivalent meaning. However, this is a very challenging task as the substitution must preserve both original meaning and grammaticality of the sentence being simplified.However, this is a very challenging task as the substitution needs to ensure grammaticality and meaning preservation. Most LS strategies in the literature are structured according to the pipeline illustrated in, which is an adaptation of the one proposed by). In this thesis, we intend to identify and address the major limitations of the approaches in the literature with respect to each step of the LS pipeline of. In an effort to create new reliable datasets for LS and to unveil information about the needs of those who can most benefit from Text Simplification, we propose new user studies with non-native speakers. We also present novel modelling strategies for each step of the LS pipeline with respect to the limitations of the approaches in the literature.",
Conclusion:,
The model is able to predict the safest path between 2 locations to a very high degree of accuracy. The accuracy of the model depends on the correct classification of the article as crime/non crime and on the correct identification of crime's location from article. Clearly the model achieves both of these with very high degrees of accuracy as can be seen from. The model also maps this safest path correctly on the map and informs the user of the route he should opt for to avoid crime prone regions.,
,
Title 1: Improving Lexical Simplification for Non-Native Speakers,
Title 2: User Studies for Lexical Simplification with Non-Native Speakers,2