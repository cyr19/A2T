,label,
ID: 970,,
Abstract:,,
"This paper proposes anew approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (""dev"") set. This profile might, for instance, be a vector with a di-mensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular sub-corpus to all the phrase pairs that can be extracted from the dev set. Then, for each phrase pair extracted from the training data, we create a vector with features defined in the same way, and calculate its similarity score with the vector representing the dev set. Thus, we obtain a decoding feature whose value represents the phrase pair's closeness to the dev. This is a simple, computationally cheap form of instance weighting for phrase pairs. Experiments on large scale NIST evaluation data show improvements over strong base-lines: +1.8 BLEU on Arabic to English and +1.4 BLEU on Chinese to English over a non-adapted baseline, and significant improvements inmost circumstances over baselines with linear mixture model adaptation. An informal analysis suggests that VSM adaptation may help in making a good choice among words with the same meaning, on the basis of style and genre.",,
Introduction:,,
"The translation models of a statistical machine translation (SMT) system are trained on parallel data. Usage of language and therefore the best translation practice differs widely across genres, topics, and dialects, and even depends on a particular author's or publication's style; the word ""domain"" is often used to indicate a particular combination of all these factors. Unless there is a perfect match between the training data domain and the (test) domain in which the SMT system will be used, one can often get better performance by adapting the system to the test domain.Domain adaptation is an active topic in the natural language processing (NLP) research community. Its application to SMT systems has recently received considerable attention. Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, instance weighting, and phrase sense disambiguation, etc.Research on mixture models has considered both linear and log-linear mixtures. Both were studied in, which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (, instead, opted for combining the sub-models directly in the SMT log-linear framework.In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (.Data selection approaches () search for bilingual sentence pairs that are similar to the indomain ""dev"" data, then add them to the training data.Instance weighting approaches () 1285 typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For example, a sentence from a subcorpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this subcorpus that appear to be of a general nature might receive higher weights.The 2012 JHU workshop on Domain Adaptation for MT 1 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system to find the appropriate translation.In this paper, we propose anew instance weighting approach to domain adaptation based on a vector space model (VSM). As in, this approach works at the level of phrase pairs. However, the VSM approach is simpler and more straightforward. Instead of using word-based features and a computationally expensive training procedure, we capture the distributional properties of each phrase pair directly, representing it as a vector in a space which also contains a representation of the dev set. The similarity between a given phrase pair's vector and the dev set vector becomes a feature for the decoder. It rewards phrase pairs that are in some sense closer to those found in the dev set, and punishes the rest. In initial experiments, we tried three different similarity functions: Bhattacharyya coefficient, Jensen-Shannon divergency, and cosine measure. They all enabled VSM adaptation to beat the non-adaptive baseline, but Bhattacharyya similarity worked best, so we adopted it for the remaining experiments.The vector space used by VSM adaptation can be defined in various ways. In the experiments described below, we chose a definition that measures the contribution (to counts of a given phrase pair, or to counts of all phrase pairs in the dev set) of each training subcorpus. Thus, the variant of VSM adaptation tested here bears a superficial resemblance to domain adaptation based on mixture models for TMs, as in, in that both approaches rely on information about the subcorpora from which the data originate. However, a key difference is that in this paper we explicitly capture each phrase pair's distribution across subcorpora, and compare it to the aggregated distribution of phrase pairs in the dev set. In mixture models, a phrase pair's distribu-tion across subcorpora is captured only implicitly, by probabilities that reflect the prevalence of the pair within each subcorpus. Thus, VSM adaptation occurs at a much finer granularity than mixture model adaptation. More fundamentally, there is nothing about the VSM idea that obliges us to define the vector space in terms of subcorpora.For instance, we could cluster the words in the source language into S clusters, and the words in the target language into T clusters. Then, treating the dev set and each phrase pair as a pair of bags of words (a source bag and a target bag) one could represent each as a vector of dimension S + T, with entries calculated from the counts associated with the S + T clusters (in away similar to that described for phrase pairs below). The (dev, phrase pair) similarity would then be independent of the subcorpora. One can think of several other ways of defining the vector space that might yield even better results than those reported here. Thus, VSM adaptation is not limited to the variant of it that we tested in our experiments.",,
Conclusion:,,
"This paper proposed anew approach to domain adaptation in statistical machine translation, based on vector space models (VSMs). This approach measures the similarity between a vector representing a particular phrase pair in the phrase table and a vector representing the dev set, yielding a feature associated with that phrase pair that will be used by the decoder. The approach is simple, easy to implement, and computationally cheap. For the two language pairs we looked at, it provided a large performance improvement over a non-adaptive baseline, and also compared 1291   favourably with linear mixture adaptation techniques. Furthermore, VSM adaptation can be exploited in a number of different ways, which we have only begun to explore. In our experiments, we based the vector space on subcorpora defined by the nature of the training data. This was done purely out of convenience: there are many, many ways to define a vector space in this situation. An obvious and appealing one, which we intend to try in future, is a vector space based on a bag-of-words topic model. A feature derived from this topicrelated vector space might complement some features derived from the subcorpora which we explored in the experiments above, and which seem to exploit information related to genre and style.",,
,,
Title 1: Domain Adaptation for SMT using a Vector Space Model,1,instance-weighting in 2 seems not crucial to me
Title 2: Instance Weighting for SMT Model Adaptation based on a Vector Space Model,,
,,
,,
ID: 1262,,
Abstract:,,
"Given the great amount of definite noun phrases that introduce an entity into the text for the first time, this paper presents a set of linguistic features that can be used to detect this type of definites in Span-ish. The efficiency of the different features is tested by building a rule-based and a learning-based chain-starting classifier. Results suggest that the classifier, which achieves high precision at the cost of recall , can be incorporated as either a filter or an additional feature within a corefer-ence resolution system to boost its performance .",,
Introduction:,,
"Although often treated together, anaphoric pronoun resolution differs from coreference resolution). Whereas the former attempts to find an antecedent for each anaphoric pronoun in a discourse, the latter aims to build full coreference chains, namely linking all noun phrases (NPs) -whether pronominal or with a nominal head -that point to the same entity. The output of anaphora resolution 1 are nounpronoun pairs (or pairs of a discourse segment and a pronoun in some cases), whereas the output of coreference resolution are chains containing a variety of items: pronouns, full NPs, discourse segments... Thus, coreference resolution requires a wider range of strategies in order to build the full chains of coreferent mentions. One of the problems specific to coreference resolution is determining, once a mention is encountered by the system, whether it refers to an entity previously mentioned or it introduces anew entity into the text. Many algorithms do not address this issue specifically, but implicitly assume all mentions to be potentially coreferent and examine all possible combinations; only if the system fails to link a mention with an already existing entity, it is considered to be chain starting. However, such an approach is computationally expensive and prone to errors, since natural language is populated with a huge number of entities that appear just once in the text. Even definite NPs, which are traditionally believed to refer to old entities, have been demonstrated to start a coreference chain over 50% of the times).An alternative line of research has considered applying a filter prior to coreference resolution that classifies mentions as either chain starting or coreferent. and have tested the impact of such a detector on the overall coreference resolution performance with encouraging results. Our chain-starting classifier is comparable -despite some differences -to the detectors suggested by,, and for English, but not identical to strictly anaphoric ones, since a non-anaphoric NP can corefer with a previous mention.This paper presents a corpus-based study of def-inite NPs in Spanish that results in a set of eight features that can be used to identify chain-starting definite NPs. The heuristics are tested by building two different chain-starting classifiers for Spanish, a rule-based and a learning-based one. The evaluation gives priority to precision over recall in view of the classifier's efficiency as a filtering module. The paper proceeds as follows. Section 2 provides a qualitative comparison with related work. The corpus study and the empirically driven set of heuristics for recognizing chain-starting definites are described in Section 3. The chain-starting classifiers are builtin Section 4. Section 5 reports on the evaluation and discusses its implications. Finally, Section 6 summarizes the conclusions and outlines future work.",,
Conclusion:,,
"The paper presented a corpus-driven chainstarting classifier of definite NPs for Spanish, pointing out and empirically supporting a series of linguistic features to betaken into account. Given that definiteness is very much language de-pendent, the AnCora-Es corpus was mined to infer some linguistic hypotheses that could help in the automatic identification of chain-starting definites. The information from different linguistic levels (lexical, semantic, morphological, syntactic, and pragmatic) in a computationally not expensive way casts light on potential features helpful for resolving coreference links. Each resulting heuristic managed to improve precision although at the cost of a drop in recall. The highest improvement in precision (89.20%) with the lowest loss in recall (78.22%) translates into an F 0.5 -measure of 85.21%. Hence, the incorporation of linguistic knowledge manages to outperform the baseline by 17 percentage points in precision. Priority is given to precision, since we want to assure that the filter prior to coreference resolution module does not label as chain starting definite NPs that are coreferent. The classifier was thus designed to minimize false positives. No less than 73% of definite NPs in the data set are chain starting, so detecting 78% of these definites with almost 90% precision could have substantial savings. From a linguistic perspective, the improvement in precision supports the linguistic hypotheses, even if at the expense of recall. However, as this classifier is not a final but a prior module, either a filter within a rule-based system or one additional feature within a larger learning-based system, the shortage of recall can be compensated at the coreference resolution stage by considering other more sophisticated features.The results here presented are not comparable with other existing classifiers of this type for several reasons. Our approach would perform differently for English, which has a lower number of definite NPs. Secondly, our classifier has been evaluated on a corpus much larger than prior ones such as. Thirdly, some classifiers aim at detecting non-anaphoric NPs, which are not the same as chain-starting. Fourthly, we have empirically explored the contribution of the set of heuristics with respect to the head_match feature. None of the existing approaches compares its final performance in relation with this simple but extremely powerful feature. Some of our heuristics do draw on previous work, but we have tuned them for Spanish and we have also contributed with new ideas, such as the use of storage units and the preference of some nouns fora specific syntactic type of modifier.As future work, we will adapt this chain-starting classifier for Catalan, fine-tune the set of heuristics, and explore to what extent the inclusion of such a classifier improves the overall performance of a coreference resolution system for Spanish. Alternatively, we will consider using the suggested attributes as part of a larger set of learning features for coreference resolution.",,
,,
Title 1: Automatic Detection of Noun Phrases Introducing Entities in Spanish,2,Chain-Started seems to be important
Title 2: A Corpus-Driven Classifier of Chain-Started Definite Noun Phrases in Spanish,,
,,
,,
ID: 1589,,
Abstract:,,
"Semantic knowledge has been adopted recently for SMT preprocessing, decoding and evaluation, in order to be able to compare sentences based on their meaning rather than on mere lexical and syntactic similarity. Little attention has been paid to semantic knowledge in the context of integrating fuzzy matches from a translation memory with SMT. We present work in progress which focuses on semantics-based pretranslation before decoding in SMT. This involves applying fuzzy matching metrics based on lexical semantics and semantic roles, aligning parse trees based on semantic roles, and pretranslating matching source sentence parts using aligned tree nodes.",,
Introduction:,,
"Semantic knowledge has been adopted recently for SMT preprocessing, decoding and evaluation. Using such knowledge helps for comparing sentences based on meaning rather than form, and for moving away from the assumption of lexical and syntactic similarity between source and target sentences. Little attention has been paid to semantic knowledge in the context of integrating fuzzy matches with SMT. Fuzzy matching methods were originally designed for translation memories, in which translators store their translations. They are now also being used in the context of SMT, for pretranslating parts of sentences before or during decoding. These methods pretranslate matching sentence parts through word alignment, parse node alignment and phrase tables, and use different degrees of linguistic knowledge.As far as we know, semantic knowledge has not yet been applied for pretranslating sentence parts before decoding in SMT. Therefore, we would like to present our work in progress, which investigates, on the one hand, the use of semantic knowledge (lexical semantics and semantic roles) for improving the usability of fuzzy matches, and, on the other hand, the pretranslation of matching sentence parts using parse nodes aligned through semantic role information.In Section 2, we provide background on fuzzy matching and on semantic knowledge in SMT, including our own previous research on fuzzy matching and tree alignment. In Section 3, we provide the methodology we are currently devising for semantics-based pretranslation. As this is work in progress, results are not yet provided. However, the discussion of our recent work on combination of fuzzy matching metrics and on semantics-based tree alignment will hint at the potential of using additional sources of linguistic information, such as lexical semantics and semantic roles, for fuzzy matching.",,
Conclusion:,,
"We have shown the main ideas of GF and how they can be applied in NLP. The most mature applications are controlled-language tasks such as dissemination translation, language teaching, and natural language queries. Such task have commercial potential, and grammars gives full control on quality. GF makes the use of grammars fea-sible with its engineering tools and its library of 30 languages. The abstract structures originally created for European languages have proven to work for Chinese as well. GF also scales up to wide-coverage translation, but is not yet competitive with statistical methods. The main advantage in this task is the compact size of the system, making it possible to use 182 language pairs off-line in a mobile device.",,
,,
Title 1: Semantics-based Pretranslation for Fuzzy Matching in SMT,2,For Fuzzy Matching in 1 seems not fully correct
Title 2: Semantics-based Pretranslation before Decoding in SMT,,
,,
,,
ID: 1862,,
Abstract:,,
"Developing Video-Grounded Dialogue Systems (VGDS), where a dialogue is conducted based on visual and audio aspects of a given video, is significantly more challenging than traditional image or text-grounded dialogue systems because (1) feature space of videos span across multiple picture frames, making it difficult to obtain semantic information; and (2) a dialogue agent must perceive and process information from different modalities (audio, video, caption, etc.) to obtain a comprehensive understanding. Most existing work is based on RNNs and sequence-to-sequence architec-tures, which are not very effective for capturing complex long-term dependencies (like in videos). To overcome this, we propose Mul-timodal Transformer Networks (MTN) to encode videos and incorporate information from different modalities. We also propose query-aware attention through an auto-encoder to extract query-aware features from non-text modalities. We develop a training procedure to simulate token-level decoding to improve the quality of generated responses during inference. We get state of the art performance on Dialogue System Technology Challenge 7 (DSTC7). Our model also generalizes to another multimodal visual-grounded dialogue task, and obtains promising performance.",,
Introduction:,,
"A video-grounded dialogue system (VGDS) generates appropriate conversational response to queries of humans, by not only keeping track of the relevant dialogue context, but also understanding the relevance of the query in the context of a given video (knowledge grounded in a video) ). An example dialogue exchange can be seen in. Developing such systems has recently received interest from the research community (e.g. DSTC7 challenge (). This task is much C: a man is standing in a kitchen putting groceries away. He closes the cabinet when finished, walks over to a table and pulls out a chair and sits down. S: a man puts away his groceries and then sits at a kitchen table and stares out the window. Q1: how many people are in the video? A1: there is just one person Q2: is there sound to the video? A2: yes there is audio but no one is talking ... Q10: is he happy or sad? A10: he appears to be neutral in expression: A sample dialogue from the DSTC7 Video Scene-aware Dialogue training set with 4 example video scenes. C: Video Caption, S: Video Summary, Qi: i th -turn question, Ai: i th -turn answer more challenging than traditional text-grounded or image-grounded dialogue systems because: (1) feature space of videos is larger and more complex than text-based or image-based features because of diverse information, such as background noise, human speech, flow of actions, etc. across multiple video frames; and (2) a conversational agent must have the ability to perceive and comprehend information from different modalities (text from dialogue history and human queries, visual and audio features from the video) and semantically shape a meaningful response to humans.Most existing approaches for multi-modal dialogue systems are based on RNNs as the sequence processing unit and sequence-to-sequence network as the overall architecture to model the sequential information in text (. Some efforts adopted query-aware attention to allow the models to focus on specific parts of the features most relevant to the dialogue context (. Despite promising results, these methods are not very effective or efficient for processing video-frames, due to the complexity of long term sequential information from multiple modalities. We propose Multimodal Transformer Networks (MTN) which model the complex sequential information from video frames, and also incorporate information from different modalities. MTNs allow for complex reasoning over multimodal data such as in videos, by jointly attending to information in different representation subspaces, and making it easier (than RNNs) to fuse information from different modalities. Inspired by the success of Transformers () for text, we propose novel neural architectures for VGDS: (1) We propose to capture complex sequential information from video frames using multi-head attention layers. Multihead attention is applied across several modalities (visual, audio, captions) repeatedly. This works like a memory network to allow the models to comprehensively reason over the video to answer human queries; (2) We propose an autoencoder component, designed as query-aware attention layer, to further improve the reasoning capability of the models on the non-text features of the input videos; and (3) We employ a training approach to improve the generated responses by simulating token-level decoding during training.We evaluated MTN on a video-grounded dialogue dataset (released through DSTC7 (). In each dialogue, video features such as audio, visual, and video caption, are available, which have to be processed and understood to hold a conversation. We conduct comprehensive experiments to validate our approach, including automatic evaluations, ablations, and qualitative analysis of our results. We also validate our approach on the visual-grounded dialogue task (, and show that MTN can generalize to other multimodal dialog systems.",,
Conclusion:,,
"In this paper, we showed that MTN, a multi-head attention-based neural network, can generate good conversational responses in multimodal settings. Our MTN models outperform the reported baseline and other submission entries to the DSTC7. We also adapted our approach to a visual dialogue task and achieved excellent performance. A possible improvement to our work is adding pre-trained embedding such as BERT ( or image-grounded word embedding ( to improve the semantic understanding capability of the models.",,
,,
Title 1: Video-Grounded Dialogue Systems with Multimodal Transformers,0,identical
Title 2: Video-Grounded Dialogue Systems with Multimodal Transformers,,
,,
,,
ID: 1947,,
Abstract:,,
"Being able to induce word translations from non-parallel data is often a prerequisite for cross-lingual processing in resource-scarce languages and domains. Previous endeavors typically simplify this task by imposing the one-to-one translation assumption, which is too strong to hold for natural languages. We remove this constraint by introducing the Earth Mover's Distance into the training of bilingual word embeddings. In this way, we take advantage of its capability to handle multiple alternative word translations in a natural form of regularization. Our approach shows significant and consistent improvements across four language pairs. We also demonstrate that our approach is particularly preferable in resource-scarce settings as it only requires a minimal seed lexicon.",,
Introduction:,,
"Bilingual lexica provide word-level semantic equivalence information across languages, and prove to be valuable fora range of cross-lingual natural language processing tasks. As building bilingual lexica from parallel corpora has been solved byword alignment, researchers have turned their attention to non-parallel corpora. Accompanied by a small seed lexicon, non-parallel corpora are usually the only resources available in resource-scarce languages and domains, making the task of bilingual lexicon induction both important and challenging. A variety of statistical methods have been proposed to induce bilingual lexica from non-parallel data. With the surge of word embeddings trained by neural networks, recent approaches that learn bilingual word representations from non-parallel data for bilingual lexicon induction have also shown promise (.However, none of the existing methods explicitly considers multiple alternative translation, i.e., the phenomenon that one source language word may have multiple possible translations in the target language. For example, the (romanized) Chinese word ""qiche"" can be translated to ""car"" or ""automobile"" in English, while the English word ""car"" can mean ""qiche"" or ""chexiang"" (railway carriage) in Chinese. Although prevalent among natural languages), multiple alternative translation is basically ignored by prior bilingual lexicon inducers; instead, they typically impose the one-to-one translation assumption) for simplicity. This represents a major drawback of existing bilingual lexicon induction approaches.There has been one study that shows potential for tackling this issue. It introduces the Earth Mover's Distance (EMD) (. Given learned bilingual word embeddings, the EMD is used as a post-processing step to match vocabularies cross-lingually, which can be interpreted as word translation. Unlike the traditional K nearest neighbors leaving the determination of the number of translation proposals K to the user, the EMD automatically determines the list of translation candidates for each source word. In this work, we propose to bring the EMD's capability to training. Intuitively, as the EMD in the post-processing step is able to connect a source word with multiple target word translations, it can play a more important role during training by driving the word vectors of these mutual translations to be closer. We therefore expect that the bilingual word embeddings learned this way will be more suitable for encoding multiple alternative translation by harnessing the power of the EMD. Our experiments validate the effectiveness of this strategy. A summary of our contributions is as follows:• We introduce the Earth Mover's Distance into the training of bilingual word embeddings, and interpret it as a natural form of regularization for the overall learning objective (Section 3).• We demonstrate significant and consistent performance improvement from our strategy across four language pairs (Sections 6.1 and 6.2).• We investigate the effect of the number of seed word translation pairs, and find our approach to be most appealing with few seeds, inline with typical resource-scarce scenarios (Section 6.3).",,
Conclusion:,,
"In this paper, we look into multiple alternative translations prevalent across natural languages, which are largely neglected in previous bilingual lexicon induction research. We propose to introduce the Earth Mover's Distance into the training of bilingual word embeddings as a natural form of regularization. We provide strong empirical results for four language pairs to demonstrate the effectiveness of our approach. Furthermore, we discover that our method remains reliable with rather few seed word translation pairs, unlike the baselines exhibiting performance degradation. This advantage of our approach is particularly desirable in realistic resource-scarce settings.",,
,,
Title 1: Bilingual Word Embeddings with Earth Mover's Distance for Cross-lingual Translation,2,Cross-lingual Translation seems not contained in the text
Title 2: Bilingual Lexicon Induction with the Earth Mover's Distance,,
,,
,,
ID: 803,,
Abstract:,,
"Multilingual writers and speakers often alternate between two languages in a single discourse , a practice called ""code-switching"". Existing sentiment detection methods are usually trained on sentiment-labeled monolingual text. Manually labeled code-switched text, especially involving minority languages, is extremely rare. Consequently, the best mono-lingual methods perform relatively poorly on code-switched text. We present an effective technique for synthesizing labeled code-switched text from labeled monolingual text, which is more readily available. The idea is to replace carefully selected subtrees of constituency parses of sentences in the resource-rich language with suitable token spans selected from automatic translations to the resource-poor language. By augmenting scarce human-labeled code-switched text with plentiful synthetic code-switched text, we achieve significant improvements in sentiment labeling accuracy (1.5%, 5.11%, 7.20%) for three different language pairs (English-Hindi, English-Spanish and English-Bengali). We also get significant gains for hate speech detection: 4% improvement using only synthetic text and 6% if augmented with real text.",,
Introduction:,,
"Sentiment analysis on social media is critical for commerce and governance. Multilingual social media users often use code-switching, particularly to express emotion ( . However, a basic requirement to train any sentiment analysis (SA) system is the availability of large sentimentlabeled corpora. These are extremely challenging to obtain), requiring volunteers fluent in multiple languages.We present CSGen, a system which provides supervised SA algorithms with synthesized unlimited sentiment-tagged code-switched text, without involving human labelers of code-switched text, or any linguistic theory or grammar for codeswitching. These texts can then train state-ofthe-art SA algorithms which, until now, primarily worked with monolingual text.A common scenario in code-switching is that a resource-rich source language is mixed with a resource-poor target language. Given a sentimentlabeled source corpus, we first create a parallel corpus by translating to the target language, using a standard translator. Although existing neural machine translators (NMTs) can translate a complete source sentence to a target sentence with good quality, it is difficult to translate only designated source segments in isolation because of missing context and lack of coherent semantics.Among our key contributions is a suite of approaches to automatic segment conversion. Broadly, given a source segment selected for codeswitching, we propose intuitive ways to select a corresponding segment from the target sentence, based on maximum similarity or minimum dissimilarity with the source segment, so that the segment blends naturally in the outer source context. Finally, the generated synthetic sentence is tagged with the same sentiment label as the source sentence. The source segment to replace is carefully chosen based on an observation that, apart from natural switching points dictated by syntax, there is a propensity to code-switch between highly opinionated segments.Extensive experiments show that augmenting scarce natural labeled code-switched text with plentiful synthetic text associated with 'borrowed' source labels enriches the feature space, enhances its coverage, and improves sentiment detection accuracy, compared to using only natural text. On four natural corpora having gold sentiment tags, we demonstrate that adding synthetic text can improve accuracy by 5.11% in English-Spanish, 7.20% in English-Bengali and (1.5%, 0.97%) in English-Hindi. The synthetic code-switch text, even when used by itself to train SA, performs almost as well as natural text in several cases. Hate speech is an extreme emotion expressed often on social media. On an EnglishHindi gold-tagged hate speech benchmark, we achieve 6% absolute F1 improvement with data augmentation, partly because synthetic text mitigates label imbalance present in scarce real text.",,
Conclusion:,,
"Code-mixing is an important and rapidly evolving mechanism of expression among multilingual populations on social media. Monolingual sentiment analysis techniques perform poorly on codemixed text, partly because code-mixed text often involves resource-poor languages. Starting from sentiment-labeled text in resource-rich source languages, we propose an effective method to synthesize labeled code-mixed text without designing switching grammars. Augmenting scarce natural text with synthetic text improves sentiment detection accuracy.",,
,,
Title 1: CSGen: Synthetic Code-Switched Text for Sentiment Analysis,1,1 contains the system title in particular
Title 2: Improving Sentiment Labeling on Manually Labeled Code-Switched Text,,
,,
,,
ID: 918,,
Abstract:,,
"We describe the semi-automatic adaptation of a TimeML annotated corpus from English to Portuguese, a language for which TimeML annotated data was not available yet. In order to validate this adaptation, we use the obtained data to replicate some results in the literature that used the original English data. The fact that comparable results are obtained indicates that our approach can be used successfully to rapidly create semantically annotated resources for new languages.",,
Introduction:,,
"Temporal information processing is a topic of natural language processing boosted by recent evaluation campaigns like TERN2004, 1 TempEval-1 () and the forthcoming TempEval-2 2. Even when the best performing systems in these competitions are symbolic, there are machine learning solutions with results close to their performance. In TempEval-1, where there were statistical and rule-based systems, almost all systems achieved quite similar results. In the TERN2004 competition (aimed at identifying and normalizing temporal expressions), a symbolic system performed best, but since then machine learning solutions, such as (, have appeared that obtain similar results.These evaluations made available sets of annotated data for English and other languages, used for training and evaluation. One natural question to ask is whether it is feasible to adapt the training and test data made available in these competitions to other languages, for which no such data still exist. Since the annotations are largely of a semantic nature, not many changes need to be done in the annotations once the textual material is translated. In essence, this would be a fast way to create temporal information processing systems for languages for which there are no annotated data yet.In this paper, we report on an experiment that consisted in adapting the English data of TempEval-1 to Portuguese. The results of machine learning algorithms over the data thus obtained are compared to those reported for the English TempEval-1 competition. There is of course the caveat that the adaptation process can introduce errors.This paper proceeds as follows. In Section 2, we provide a quick overview of the TimeML annotations in the TempEval-1 data. In Section 3, it is described how the data were adapted to Portuguese. Section 4 contains a brief quantitative comparison of the two corpora. In Section 5, the results of replicating one of the approaches present in the TempEval-1 challenge with the Portuguese data are presented. We conclude this paper in Section 6. contains an example of a document from the TempEval-1 corpus, which is similar to the TimeBank corpus (.",,
Conclusion:,,
"Current SRL approaches limit the search for arguments to the sentence containing the predicate of interest. Many systems take this assumption a step further and restrict the search to the predicate's local syntactic environment; however, predicates and the sentences that contain them rarely exist in isolation. As shown throughout this paper, they are usually embedded in a coherent and semantically rich discourse that must betaken into account. We have presented a preliminary study of implicit arguments for nominal predicates that focused specifically on this problem. First, we have created gold-standard implicit argument annotations fora small set of pervasive nominal predicates. Our analysis shows that these annotations add 65% to the role coverage of NomBank. Second, we have demonstrated the feasibility of recovering implicit arguments for many of the predicates, thus establishing a baseline for future work on this emerging task. Third, our study suggests a few ways in which this research can be moved forward. As shown in Section 6, many errors were caused by the absence of true implicit arguments within the set of candidate constituents. More intelligent windowing strategies in addition to alternate candidate sources might offer some improvement. Although we consistently observed development gains from using automatic coreference resolution, this process creates errors that need to be studied more closely. It will also be important to study implicit argument patterns of non-verbal predicates such as the partitive percent. These predicates are among the most frequent in the TreeBank and are likely to require approaches that differ from the ones we pursued.Finally, any extension of this work is likely to encounter a significant knowledge acquisition bottleneck. Implicit argument annotation is difficult because it requires both argument and coreference identification (the data produced by is similar). Thus, it might be productive to focus future work on (1) the extraction of relevant knowledge from existing resources (e.g., our use of coreference patterns from Gigaword) or (2) semi-supervised learning of implicit argument models from a combination of labeled and unlabeled data.",,
,,
Title 1: Adapting a TimeML annotated corpus from English to Portuguese,1,1 is more specific
Title 2: Adapting a TimeML Annotated Corpus to Portuguese,,
,,
,,
ID: 894,,
Abstract:,,
"This paper describes speech translation from Amharic-to-English, particularly Automatic Speech Recognition (ASR) with post-editing feature and Amharic-English Statistical Machine Translation (SMT). ASR experiment is conducted using morpheme language model (LM) and phoneme acoustic model (AM). Likewise, SMT conducted using word and morpheme as unit. Morpheme based translation shows a 6.29 BLEU score at a 76.4% of recognition accuracy while word based translation shows a 12.83 BLEU score using 77.4% word recognition accuracy. Further, after post-edit on Amharic ASR using corpus based n-gram, the word recognition accuracy increased by 1.42%. Since post-edit approach reduces error propagation, the word based translation accuracy improved by 0.25 (1.95%) BLEU score. We are now working towards further improving propagated errors through different algorithms at each unit of speech translation cascading component.",,
Introduction:,,
"Speech is one of the most natural form of communication for humankind. Computer with the ability to understand natural language promoted the development of man-machine interface. This can be extended through different digital platforms such as radio, mobile, TV, CD and others. Through these, speech translation facilitates communication between the people who speak different languages.Speech translation is the process by which spoken source phrases are translated to a target language using a computer (). Speech translation research for major and technological supported languages like English, European languages (like French and Spanish) and Asian languages (like Japanese and Chinese) has been conducted since the 1983s by NEC Corporation. The advancement of speech translation captivates the communication between people who do not share the same language.The state-of-the-art of speech translation system can be seen as the integration of three major cascading components (; Automatic Speech Recognition (ASR), Machine Translation (MT) and Text-ToSpeech (TTS) synthesis. ASR is the process by which a machine infers spoken words, by means of talking to computer, and having it correctly understand a recorded audio signal. Beside ASR, MT is the process by which a machine is used to translate a text from one source language to another target language. Finally, TTS creates a spoken version from the text of electronic document such as text file and web document.As one major component of speech translation, Amharic ASR started in. A number of attempts have been made for Amharic ASR using different methods and techniques towards designing speaker independent, large vocabulary, contineous speech and spontaneous speech recognition.In addition to ASR, a preliminary EnglishAmharic machine translation experiments was conducted using phonemic transcription on the Amharic corpus (). The result obtained from the experiment shows that, it is possible to design English-Amharic machine translation using statistical method.As the last component of speech translation, a number of TTS research have been attempted using different techniques and methods as discussed by). Among these, concatenative, cepstral, formant and a syllable based speech synthesizers were the main methods and techniques applied.All the above research works were conducted using different methods and techniques beside data difference and integration as a cascading component. Moreover, dataset and tools used in the above research are not accessible which makes difficult to evaluate the advancement of research in speech technology for local languages.However, there is no attempt to integrate ASR, SMT and TTS to come up with speech translation system for Amharic language. Thus, the main aim of this study is to investigate the possibility to design Amharic-English speech translation system that controls recognition errors propagating through cascading components.",,
Conclusion:,,
"We have presented the methods, the data, the evaluation setup, and the results for four shared tasks taht we organized as part of the VarDial 2017 evaluation campaign. To the best of our knowledge, this is the first comprehensive evaluation campaign on NLP for Similar Languages, Varieties and Dialects. Three tasks (ADI, GDI, and DSL) dealt with dialect and language variety identification, focusing on Arabic, German and several groups of similar languages, respectively, whereas the CLP task dealt with parsing.Along with the results of each shared task, we also included short descriptions of each participating system in order to provide readers with an overview of all approaches proposed for each task. For a complete description of each system, we included references to the fifteen system description papers that were accepted for presentation at the VarDial workshop at EACL'2017.Given the success of the VarDial evaluation campaign, we believe that there is room for another edition with more shared tasks. Possible topics of interest for future shared tasks include machine translation between similar languages and POS tagging of dialects, among others.",,
,,
Title 1: Automatic Speech Recognition and Statistical Machine Translation for Amharic Language,2,2 is more specific and precise
Title 2: Automatic Speech Translation from Amharic-English using Post-Editing Feature and Statistical Machine Translation,,
,,
,,
ID: 1080,,
Abstract:,,
"We use machine learners trained on a combination of acoustic confidence and pragmatic plausi-bility features computed from dialogue context to predict the accuracy of incoming n-best recognition hypotheses to a spoken dialogue system. Our best results show a 25% weighted f-score improvement over a baseline system that implements a ""grammar-switching"" approach to context-sensitive speech recognition.",,
Introduction:,,
"A crucial problem in the design of spoken dialogue systems is to decide for incoming recognition hypotheses whether a system should accept (consider correctly recognized), reject (assume misrecognition), or ignore (classify as noise or speech not directed to the system) them. In addition, a more sophisticated dialogue system might decide whether to clarify or confirm certain hypotheses.Obviously, incorrect decisions at this point can have serious negative effects on system usability and user satisfaction. On the one hand, accepting misrecognized hypotheses leads to misunderstandings and unintended system behaviors which are usually difficult to recover from. On the other hand, users might get frustrated with a system that behaves too cautiously and rejects or ignores too many utterances. Thus an important feature in dialogue system engineering is the tradeoff between avoiding task failure (due to misrecognitions) and promoting overall dialogue efficiency, flow, and naturalness.In this paper, we investigate the use of machine learners trained on a combination of acoustic confidence and pragmatic plausibility features (i.e. computed from dialogue context) to predict the quality of incoming n-best recognition hypotheses to a spoken dialogue system. These predictions are then used to select a ""best"" hypothesis and to decide on appropriate system reactions. We evaluate this approach in comparison with a baseline system that combines fixed recognition confidence rejection thresholds with dialogue-state dependent recognition grammars.The paper is organized as follows. After a short relation to previous work, Section 3 introduces the WITAS multimodal dialogue system, which we use to collect data (Section 4) and to derive baseline results (Section 5). Section 6 describes our learning experiments for classifying and selecting from nbest recognition hypotheses and Section 7 reports our results.",,
Conclusion:,,
"We used a combination of acoustic confidence and pragmatic plausibility features (i.e. computed from dialogue context) to predict the quality of incoming recognition hypotheses to a multi-modal dialogue system. We classified hypotheses as accept, (clarify), reject, or ignore: functional categories that can be used by a dialogue manager to decide appropriate system reactions. The approach is novel in combining machine learning with n-best processing for spoken dialogue systems using the Information State Update approach.Our best results, obtained using TiMBL with optimized parameters, show a 25% weighted f-score improvement over a baseline system that uses a ""grammar-switching"" approach to context-sensitive speech recognition, and are only 8% away from the optimal performance that can be achieved on the data. Clearly, this improvement would result in better dialogue system performance overall. Parameter optimization improved the classification results by 9% compared to using the learner with default settings, which shows the importance of such tuning.Future work points in two directions: first, integrating our methodology into working ISU-based dialogue systems and determining whether or not they improve in terms of standard dialogue evaluation metrics (e.g. task completion). The ISU approach is a particularly useful testbed for our methodology because it collects information pertaining to dialogue context in a central data structure from which it can be easily extracted. This avenue will be further explored in the TALK project 8 . Second, it will be interesting to investigate the impact of different dialogue and task features for classification and to introduce a distinction between ""generic"" features that are domain independent and ""application-specific"" features which reflect properties of individual systems and application scenarios.",,
,,
Title 1: Using Acoustic Confidence and Pragmatic Plausibility to Classify N-Best Recognition Hypotheses to a Multi-Modal Dialogue System,1,2 is too unspecific
Title 2: Context-Sensitive Speech Recognition in Spoken Dialogue Systems,,
,,
,,
ID: 299,,
Abstract:,,
"As the volume of documents on the Web increases , technologies to extract useful information from them become increasingly essential. For instance, information extracted from social network services such as Twitter and Facebook is useful because it contains a lot of location-specific information. To extract such information, it is necessary to identify the location of each location-relevant expression within a document. Previous studies on location disambiguation have tackled this problem on the basis of word sense disambigua-tion, and did not make use of location-specific clues. In this paper, we propose a method for location disambiguation that takes advantage of the following two clues: spatial proximity and temporal consistency. We confirm the effectiveness of these clues through experiments on Twitter tweets with GPS information.",,
Introduction:,,
"As the volume of documents on the Web increases, technologies to extract useful information from them become increasingly essential. For instance, information extracted from social network services (SNS) such as Twitter and Facebook is useful because it contains a lot of location-specific information. To extract such information, it is necessary to identify the location of each location-relevant expression within a document.However, many previous studies on SNS rely only on geo-tagged documents (e.g.,), which include GPS information, but these represent only a small proportion of the total. To extract as much location information as possible, it is important to develop a method that can estimate locations from numerous documents without GPS information.Previous studies on location disambiguation made use of methods for word sense disambiguation and are based only on textual information, i.e., the bagof-words in a document. It is, however, difficult to solve this problem using only textual information in a relatively short SNS document. For example, it is difficult to identify the location of ""Prefectural Office Ave."" from the following document based only on word information. ""I arrived at Prefectural Office Ave. from Shuri Station!""In this paper, we propose a method that identifies the locations of location expressions in Twitter tweets on the basis of the following two clues: (1) spatial proximity, and (2) temporal consistency. Spatial proximity assumes that all locations mentioned in a tweet are close to one another. In the above document, for example, we would assume that ""Prefectural Office Ave."" is ""Prefectural Office Ave. (Okinawa)"" using the proximity between ""Shuri Station"" and ""Prefectural Office Ave. (Okinawa)"" The other clue is temporal consistency, Semiocast reported that GPS information is assigned to only 0.77% of all public tweets. Although it is possible to learn a clue from ""Shuri Station,"" which is located in Okinawa Prefecture, it would require a large amount of training data to learn such lexical clues for each target location expression.1 which assumes that the locations in a series of tweets are near to each other.In our experiments, we learn a location classifier for each ambiguous location expression in Japanese. Hereafter, we call an ambiguous location expression, such as ""Prefectural Office Ave.,"" a Location EXpression (LEX), and a location to which a LEX points, such as <Prefectural Office Ave. (Okinawa)>, a Location Entity (LE), which is linked to its GIS information. We calla LEX linked to multiple LEs an ambiguous LEX, which is the target of our location name disambiguation system. That is unambiguous LEXs are not our target, such as ""Tokyo Tower,"" which points the LE <Tokyo Tower>.We define a set of LEXs and LEs on the basis of Japanese Wikipedia. Training data for the location classifiers are created from tweets containing GPS information. The resulting location classifiers can be applied to LEXs in any tweets or documents without GPS information.Our novel contributions can be summarized as follows:• two novel clues for location disambiguation are proposed, • training data is automatically created from tweets with GPS information, and • our method can identify LEs of LEXs in any documents without GPS information.The remainder of this paper is organized as follows. Section 2 introduces related work, while Section 3 describes the resources used in this paper. Section 4 details our proposed method and Section 5 reports the experimental results. Section 6 concludes the paper.",,
Conclusion:,,
"In this paper, we presented a method for location name disambiguation for text snippets on SNS. We considered both the spatial proximity and temporal consistency to produce the estimates of LEs. As a result, our method substantially outperformed the baseline method that considers only lexical information. More specifically:• Considering the spatial proximity improves the accuracy• Considering the temporal consistency with many tweets improves the accuracy• Considering both of the above outperforms the baseline by 7.13%In future work, first, we plan to further investigate the cause of the decrease inaccuracy when the temporal consistency feature considers many tweets.Second, in this paper, only tweets including unambiguous LEXs are used to calculate the proximity feature for the target LEX. However, tweets including ambiguous LEXs could also be used if the LEXs have been disambiguated in advance.In addition, we estimated the LEs of ambiguous LEXs, although the location estimation has several problems. One concerns whether the user posting the tweet including the LEX is actually at that location. Solving this problem is necessary for some applications specializing in GIS information. In future work, we aim to solve this problem using the proposed spatial proximity and temporal consistency.",,
,,
Title 1: Location Disambiguation Using Spatial Clustering and Temporal Consistency,2,clustering is hallucinated
Title 2: Location Disambiguation Using Spatial and Temporal Clues,,
,,
,,
ID: 1109,,
Abstract:,,
"Collection of natural language texts in to a machine readable format for investigating various linguistic phenomenons is calla corpus. A well structured corpus can help to know how people used that language in day-today life and to build an intelligent system that can understand natural language texts. Here we review our experience with building a corpus containing 1.5 million words of Bodo language. Bodo is a Sino Tibetan family language mainly spoken in Northern parts of Assam, the North Eastern state of India. We try to improve the quality of Bodo corpora considering various characteristics like representativeness, machine readability, finite size etc. Since Bodo is one of the Indian language which is lesser reach on literary and computationally we face big problem on collecting data and our generated corpus will help the researchers in both field.",,
Introduction:,,
"The term corpus is derived from Latin corpus ""body"" which it means as a representative collection of texts of a given language, dialect or other subset of a language to be used for linguistic analysis. Precisely, it refers to (a) (loosely) anybody of text; (b) (most commonly) a body of machine readable text; and (c) (more strictly) a finite collection of machine-readable texts sampled to be representative of a language or variety. Again, Corpus is a machine readable texts (both spoken and written) document stored in machine systematically collected from different sources. It is an important text in digital media world. It is defined as corpus and in plural corpora a collection of linguistic data, either compiled as written texts or as a transcription of recorded speech. The main purpose of a corpus is to verify a hypothesis about language -for example, to determine how the usage of a particular sound, word, or syntactic construction varies. Corpus linguistics deals with the principles and practice of using corpora in language study. A computer corpus is a large body of machine-readable texts . So it is the computerization of varieties text (various domains of texts such as literature, science, sports etc.) of a given language. Corpus maybe of monolingual, bilingual and multilingual format of machine readable data etc. It is an annotated and tagged component of parts of speech. It is most important for computing to make it accessible worldwide via internet. Moreover it is a valid machine readable data of a given language which gives us proper information of a language where it follows linguistics principles. The need of language corpora has given rise to the study of corpus linguistics. It is not a branch of linguistics but the methodology that helps in analysis and research of naturally occurring language through the help of computerized corpora, i.e. with the specialized software. From the very beginning, modern corpus linguistics has been closely associated with the development of computer software for corpus analysis. In modern corpus linguistics, the linguists and the computer scientists share a common goal that it is important to depend on the real or actual language data (speech or written) for carrying out any kind of linguistic analysis. Moreover, it is an approach which satisfies two main purposes: how people use language in day-to-day communication and to buildup intelligent system to interact with human beings. It is not easy to classify corpora into various types. Modern day corpora are of various types. In fact, it is a very crucial task of classifying language corpora into different types. However, written corpus, spoken corpus, general corpus, monolingual corpus, bilingual corpus, unannotated corpus, annotated corpus, parallel and learner corpus are worth mentioning.",,
Conclusion:,,
"It is seen from the above discussion that there is no developed fonts in Bodo. Due to in-uniformity of spelling the compiler of the corpus has to face several problems while entering the text into the format. In such cases they have to correct themselves. There is no science and sentimental fictions in Bodo and in some fields like journals like women's, children's, whether it is monthlies, bi-monthlies and news papers whether it is dailies, weeklies etc are very rare. The entire generation of Bodo corpus is based the standardization of trio-lingual (a Bodo-English-Hindi) dictionary of Bodo Sahitya Sabha published by Onsumwi Library, Kokrajhar, Assam in some cases and linguistics standardization is also followed. Validation of this generation corpus is done manually as this language does not have still tagged corpus and annotated texts.",,
,,
Title 1: Building a Bodo Corpus for Linguistic Analysis,1,phenomenons is not grammatical (taken from the abstract)
Title 2: Building a corpus of Bodo language for studying linguistic phenomenons,,
,,
,,
ID: 1418,,
Abstract:,,
"Texts from the Internet serve as important data sources for financial market modeling. Early statistical approaches rely on manually defined features to capture lexical, sentiment and event information, which suffers from feature sparsity. Recent work has considered learning dense representations for news titles and abstracts. Compared to news titles, full documents can contain more potentially helpful information, but also noise compared to events and sentences, which has been less investigated in previous work. To fill this gap, we propose a novel target-specific abstract-guided news document representation model. The model uses a target-sensitive representation of the news abstract to weigh sentences in the news content, so as to select and combine the most informative sentences for market modeling. Results show that document representations can give better performance for estimating cumulative abnormal returns of companies when compared to titles and abstracts. Our model is especially effective when it used to combine information from multiple document sources compared to the sentence-level baselines.",,
Introduction:,,
"Texts from the Internet was shown to be statistically correlated with stock market trends (). Natural language processing (NLP) techniques have been applied to extract information from company filings (), financial news articles () and social media texts) in order to gain understandings of financial markets. In particular, traditional methods exploited statistical signals, such as lexical and syntactic features (), sentiment) and event structures () from these text sources, which suffers from feature sparsity problems.With the recent trends in deep learning for NLP, neural networks have also been leveraged to learn dense representations for text elements, which can address the sparsity of discrete features in statistical models. Such representations can implicitly represent sentiment, event and factual information, which can be extremely challenging for sparse indicator features. In particular, show that deep learning representations of event structures yield better accuracies for stock market prediction compared to discrete event features., use neural networks to directly learn representations of news abstracts, showing that it is effective for predicting the cumulative abnormal returns of public companies.One limitation of and, however, is that these methods only model news titles and abstract texts, which are typically single sentences. show that a model that uses only news content gives inferior results compared to one trained using news titles only, and that adding news content information to a title-driven model does not significantly improve the results. Intuitively, news content can contain richer information that is not directly relevant to the title message, or the most important event, and hence can lead to noise in predictive modeling. On the other hand, news content can also contain useful information for making informed decisions. For example, given the news abstract passage ""Amicus Inc (arcs.o), a provider of health-care IT services, said it agreed to be bought by an affiliate of private firm them bravo llc for $217 million in an all-crash deal."", it would be difficult to tell whether the acquisition is beneficial to investors. However, a sentence in the news content states that ""the deal offers Amicus Inc shareholders $5.35 for each share, a premium of 21 percent over the stock's close on December 24 on NASDAQ"", which explicitly indicates a positive return.We aim to exploit such useful information from the news content for making more informed decisions in stock market prediction. A main challenge is how to automatically identify the most useful parts of the news content, while disregarding noise, which prevents naive utilization of news contexts (aka). Another challenge, as suggest, is that information must be selected with regard to a certain stock of interest. As shown in, the same event ""Salesforce rules out Twitter bid"" can lead to different influences on different stocks, with Salesforce benefiting from it yet Twitter suffering from it.To address the above challenges, we build a neural model that selects and represents relevant sentences from a full news document with respect to a specific firm of interest. In particular, we leverage conditional encoding to encode information of a given stock into the dense representation of the news abstract. Using this target-specific news abstract representation, we apply neural attention () over each sentence in the news content to automatically learn its relative importance with regard to the target company. The attention weights are learned automatically towards a final predictive goal. The model is full data-driven, which do not rely on an external syntactic parser as the first step to obtain its target-specific linguistic structures.In addition to model, which uses only abstract information, we also compare with several state-of-the-art baselines for learning document representations, such as paragraph vector) and hierarchical attention network (, giving the best reported performances. The advantage of our approach over sentence-level baseline is especially obvious when it is used to combine information from multiple news document sources. In addition, a case study shows that our model can select the sentences that most intuitively help predict stock returns from a full news document. Our contributions can be summarized as follows:• We propose a target-specific document representation model, which leverages the abstracts as evidences to select informative sentences from the documents while disregarding noise.• We are the first, to our knowledge, to build a neural model that can effectively leverage full news document for stock market prediction.Resources of this work can be found at http://github.com/sudy/coling20182 Problem Definition Cumulative Abnormal Return The task that we attack in this paper is Cumulative Abnormal Return Prediction (CAR). Formally, the abnormal return AR jt of a firm j on a date t is the difference between its actual return R jt and the expected returnˆRreturnˆ returnˆR jt , AR jt = R jt − ˆ R jt . The expected returnˆRreturnˆ returnˆR jt can be estimated by an asset price model based on historical prices, or approximated by the market return in a short-term event window (). Given a document D and associated firm f , we learn a representation d f for D that is specified to firm f based on our proposed approach (introduced in Section 3). With the firm-specific document representation d f , we predict the CAR 3 direction y ∈ {−1, 1} using a parameterized softmax function (Eq 2).where p(y = −1|d f ) and p(y = 1|d f ) indicate the probability of CAR 3 being negative and positive, respectively. In the next section, we describe our method for learning d f .",,
Conclusion:,,
"We investigated target-specific document representations of financial news for cumulative abnormal return prediction in this paper, comparing a set of document embedding models. Empirical studies showed that a combination of target-specific news abstract representation and contextual sentence representation via the attention mechanism can give better results compared to several alternative sentence-level and document-level methods. Our final model demonstrated the usefulness of modeling a full document, as compared to only titles and abstracts, for obtaining more accurate results.",,
,,
Title 1: A Target-Specific Abstract-Guided News Document Representation for Financial Market Modeling,2,last sentence of introduction helps
Title 2: A Target-Specific Abstract-Guided News Document Representation Model for Stock Market Prediction,,
,,
,,
ID: 1,,
Abstract:,,
"Opinion mining is often regarded as a classification or segmentation task, involving the prediction of i) subjective expressions, ii) their target and iii) their polarity. Intuitively , these three variables are bidirec-tionally interdependent, but most work has either attempted to predict them in isolation or proposing pipeline-based approaches that cannot model the bidirectional interaction between these variables. Towards better understanding the interaction between these variables, we propose a model that allows for analyzing the relation of target and subjective phrases in both directions, thus providing an upper bound for the impact of a joint model in comparison to a pipeline model. We report results on two public datasets (cameras and cars), showing that our model outperforms state-of-the-art models, as well as on anew dataset consisting of Twitter posts.",,
Introduction:,,
"Sentiment analysis or opinion mining is the task of identifying subjective statements about products, their polarity (e. g. positive, negative or neutral) in addition to the particular aspect or feature of the entity that is under discussion, i. e., the socalled target. Opinion analysis is thus typically approached as a classification) or segmentation ( task by which fragments of the input are classified or labelled as representing a subjective phrase, a polarity or a target (. However, such pipeline models do not allow for inclusion of bidirectional interactions between the key variables. In this paper, we propose a model that can include bidirectional dependencies, attempting to answer the following questions which so far have not been addressed but provide the basis fora joint model:• What is the impact of the performance loss of a non-perfect subjective term extraction in comparison to perfect knowledge? • Further, how does perfect knowledge about targets influence the prediction of subjective terms? • How is the latter affected if the knowledge about targets is imperfect, i. e. predicted by a learned model?We study these questions using imperatively defined factor graphs (IDFs,,) to show how these bidirectional dependencies can be modeled in an architecture which allows for further steps towards joint inference. IDFs area convenient way to define probabilistic graphical models that make structured predictions based on complex dependencies.",,
Conclusion:,,
We have presented an NLG system that can generate ecologically informative and engaging narratives of animal (red kite) movements. Our initial evaluations have shown encouraging results and further evaluations are now planned. The system can be accessed through http://redkite.abdn.ac.uk/blog/. This week Millie did not travel far but was actively exploring a small area north-east of Loch Ness. Friday morning Millie left the woodland where she spend the night to fly to Loch Ruthven amid heavy rain. The poor visibility may have driven her to fly low when searching for food along the water sides. Early that evening she was seen in farmland near Torness.,,
,,
Title 1: A Joint Model for Bidirectional Interaction between Target and Subjective Phrases in Opinion Mining,1,2 is non-grammatical or at least undesirably repetitive
"Title 2: A Joint Model for Bidirectional Interaction between Subjective, Target and Subjective Expressions in Opinion Mining",,
,,
,,
ID: 18,,
Abstract:,,
"We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outper-forms Pharaoh, a state-of-the-art decoder for phrase-based models.",,
Introduction:,,
"Phrase-based translation models (), which go beyond the original IBM translation models 1 by modeling translations of phrases rather than individual words, have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations.In phrase-based models, phrases are usually strings of adjacent words instead of syntactic constituents, excelling at capturing local reordering and performing translations that are localized to The mathematical notation we use in this paper is taken from that paper: a source string f J 1 = f1, . . , fJ is to be translated into a target string e I 1 = e1, . . , eI . Here, I is the length of the target string, and J is the length of the source string.substrings that are common enough to be observed on training data. However, a key limitation of phrase-based models is that they fail to model reordering at the phrase level robustly. Typically, phrase reordering is modeled in terms of offset positions at the word level), making little or no direct use of syntactic information.Recent research on statistical machine translation has lead to the development of syntax-based models. proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. represent each production in parallel dependency tree as a finite transducer. formalizes machine translation problem as synchronous parsing based on multitext grammars. describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of asynchronous context-free grammar. propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, aversion of synchronous grammars defined on dependency trees. All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both source and target languages.Another class of approaches make use of syntactic information in the target language alone, treating the translation problem as a parsing problem. use a parser in the target language to train probabilities on a set of operations that transform a target parse tree into a source string.Paying more attention to source language analysis, employ a source language dependency parser, a target language word segmentation component, and an unsupervised word alignment component to learn treelet translations from parallel corpus.In this paper, we propose a statistical translation model based on tree-to-string alignment template which describes the alignment between a source parse tree and a target string. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string.One advantage of our model is that TATs can be automatically acquired to capture linguistically motivated reordering at both low (word) and high (phrase, clause) levels. In addition, the training of TAT-based model is less computationally expensive than tree-to-tree models. Similarly to (), the tree-to-string alignment templates discussed in this paper are actually transformation rules. The major difference is that we model the syntax of the source language instead of the target side. As a result, the task of our decoder is to find the best target string while Galley's is to seek the most likely target tree.",,
Conclusion:,,
"In this paper, we introduce tree-to-string alignment templates, which can be automatically learned from syntactically-annotated training data. The TAT-based translation model improves translation quality significantly compared with a stateof-the-art phrase-based decoder. Treated as special TATs without tree on the source side, bilingual phrases can be utilized for the TAT-based model to get further improvement.It should be emphasized that the restrictions we impose on TAT extraction limit the expressive power of TAT. Preliminary experiments reveal that removing these restrictions does improve translation quality, but leads to large memory requirements. We feel that both parsing and word alignment qualities have important effects on the TATbased model. We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in ( ).",,
,,
Title 1: Tree-to-String Alignment Template for Statistical Machine Translation,1,"Even though ""2"" could be seen as ""more correct English"", in a scientific context, I slightly prefer ""1"""
Title 2: A Tree-to-string Alignment Template for Machine Translation,,
,,
,,
ID: 671,,
Abstract:,,
"In this paper, we propose anew methodology based on directed graphs and the TextRank algorithm to automatically induce general-specific noun relations from web corpora frequency counts. Different asymmetric association measures are implemented to build the graphs upon which the TextRank algorithm is applied and produces an ordered list of nouns from the most general to the most specific. Experiments are conducted based on the WordNet noun hierarchy and assess 65.69% of correct word ordering.",,
Introduction:,,
"Taxonomies are crucial for any knowledgebased system. They are in fact important because they allow to structure information, thus fostering their search and reuse. However, it is well known that any knowledge-based system suffers from the so-called knowledge acquisition bottleneck, i.e. the difficulty to actually model the domain in question. As stated in, WordNet has been an important lexical knowledge base, but it is insufficient for domain specific texts. Most of the works proposed so far have (1) used predefined patterns or (2) automatically learned these patterns to identify hypernym/hyponym relationships. From the first paradigm, first identifies a set of lexico-syntactic patterns that are easily recognizable i.e. occur frequently and across text genre boundaries. These can be called seed patterns. Based on these seeds, she proposes a bootstrapping algorithm to semi-automatically acquire new more specific patterns. Similarly,) uses predefined patterns such as ""X is a kind of Y"" or ""X, Y, and other Zs"" to identify hypernym/hyponym relationships. This approach to information extraction is based on a technique called selective concept extraction as defined by. Selective concept extraction is a form of text skimming that selectively processes relevant text while effectively ignoring surrounding text that is thought to be irrelevant to the domain. A more challenging task is to automatically learn the relevant patterns for the hypernym/hyponym relationships. In the context of pattern extraction, there exist many approaches as summarized in). The most well-known work in this area is certainly the one proposed by ) who use machine learning techniques to automatically replace hand-built knowledge. By using dependency path features extracted from parse trees, they introduce a general-purpose formalization and generalization of these patterns. Given a training set of text containing known hypernym pairs, their algorithm automatically extracts useful dependency paths and applies them to new corpora to identify novel pairs. () use a similar way as () to derive extraction patterns for hypernym/hyponym relationships by using web search engine counts from pairs of words encountered in WordNet. However, the most interesting work is certainly proposed by) who extract patterns in two steps. First, they find lexical relationships between synonym pairs based on snippets counts and apply wildcards to generalize the acquired knowledge. Then, they apply a SVM classifier to determine whether anew pair shows a relation of synonymy or not, based on a feature vector of lexical relationships. This technique could be applied to hypernym/hyponym relationships although the authors do not mention it. On the one hand, links between words that result from manual or semi-automatic acquisition of relevant predicative or discursive patterns) are fine and accurate, but the acquisition of these patterns is a tedious task that requires substantial manual work. On the other hand, works done by) have proposed methodologies to automatically acquire these patterns mostly based on supervised learning to leverage manual work. However, training sets still need to be built. Unlike other approaches, we propose an unsupervised methodology which aims at discovering general-specific noun relationships which can be assimilated to hypernym/hyponym relationships detection 2 . The advantages of this approach are clear as it can be applied to any language or any domain without any previous knowledge, based on a simple assumption: specific words tend to attract general words with more strength than the opposite. state: ""there is a tendency fora strong forward association from a specific term like adenocarcinoma to the more general term cancer, whereas the association from cancer to adenocarcinoma is weak"". Based on this assumption, we propose a methodology based on directed graphs and the TextRank algorithm () to automatically induce general-specific noun relationships from web corpora frequency counts. Indeed, asymmetry in Natural Language Processing can be seen as a possible reason for the degree of generality of terms (. So, different asymmetric association measures are implemented to build the graphs upon which the TextRank algorithm is applied and produces an ordered list of nouns, from the most general to the most specific. Experiments have been conducted based on the WordNet noun hierarchy and assessed that 65% of the words are ordered correctly.",,
Conclusion:,,
"In this paper, we proposed anew methodology based on directed weighted/unweighted graphs and the TextRank algorithm to automatically in- Finalized by duce general-specific noun relationships from web corpora frequency counts. To our knowledge, such an unsupervised experiment has never been attempted so far. In order to evaluate our results, we proposed three different evaluation metrics. The results obtained by using seven asymmetric association measures based on web frequency counts showed promising results reaching levels of (1) constraint coherence of 65.69%, (2) clustering mapping of 59.50% in terms of precision for the hypernym level and 42.72% on average in terms of f-measure and ranking similarity of 0.39 for the Spearman's rank correlation coefficient. As future work, we intend to take advantage of the good performance of our approach at the hypernym level to propose a recursive process to improve precision results overall levels of generality. Finally, it is important to notice that the evaluation by clustering evidences more than a simple evaluation of the word order, but shows how this approach is capable to automatically map clusters to WordNet classification.",,
,,
Title 1: Automatic Induction of General-Specific Noun Relationships from Web Corpora,2,"Hard to decide for me, but 2 (verb) looks more modern"
Title 2: Automatically Inducing General-Specific Noun Relations from Web Corpora,,
,,
,,
ID: 2231,,
Abstract:,,
"We address the problem dealing with a large collection of data, and investigate the use of automatically constructing category hierarchy from a given set of categories to improve classification of large corpora. We use two well-known techniques, partitioning clustering, 񮽙-means and a ÐÓ×× ÙÒÒØØÓÒ to create category hierarchy. 񮽙-means is to cluster the given categories in a hierarchy. To select the proper number of 񮽙, we use a ÐÓ×× ÙÒÒØØÓÒ which measures the degree of our disappointment in any differences between the true distribution over inputs and the learner's prediction. Once the optimal number of 񮽙 is selected, for each cluster , the procedure is repeated. Our evaluation using the 1996 Reuters corpus which consists of 806,791 documents shows that automatically constructing hierarchy improves classification accuracy.",,
Introduction:,,
"Text classification has an important role to play, especially with the recent explosion of readily available online documents. Much of the previous work on text classification use statistical and machine learning techniques. However, the increasing number of documents and categories often hamper the development of practical classification systems, mainly by statistical, computational, and representational problems). One strategy for solving these problems is to use category hierarchies. The idea behind this is that when humans organize extensive data sets into fine-grained categories, category hierarchies are often employed to make the large collection of categories more manageable.McCallum et. al. presented a method called 'shrinkage' to improve parameter estimates by taking advantage of the hierarchy). They tested their method using three different real-world datasets: 20,000 articles from the UseNet, 6,440 web pages from the Industry Sector, and 14,831 pages from the Yahoo, and showed improved performance. Dumais et. al. also described a method for hierarchical classification of Web content consisting of 50,078 Web pages for training, and 10,024 for testing, with promising results). Both of them use hierarchies which are manually constructed. Such hierarchies are costly human intervention, since the number of categories and the size of the target corpora are usually very large. Further, manually constructed hierarchies are very general in order to meet the needs of a large number of forthcoming accessible source of text data, and sometimes constructed by relying on human intuition. Therefore, it is difficult to keep consistency, and thus, problematic for classifying text automatically.In this paper, we address the problem dealing with a large collection of data, and propose a method to generate category hierarchy for text classification. Our method uses two well-known techniques, partitioning clustering method called 񮽙-means and a ÐÓ×× ÙÒÒØØÓÒ to create hierarchical structure. 񮽙-means partitions a set of given categories into 񮽙 clusters, locally minimizing the average squared distance between the data points and the cluster centers. The algorithm involves iterating through the data that the system is permitted to classify during each iteration and constructs category hierarchy. To select the proper number of 񮽙 during each iteration, we use a ÐÓ×× ÙÒÒØØÓÒ which measures the degree of our disappointment in any differences between the true distribution over inputs and the learner's prediction. Another focus of this paper is whether or not a large collection of data, the 1996 Reuters corpus helps to generate a category hierarchy which is used to classify documents.The rest of the paper is organized as follows. The next section presents a brief review the earlier work. We then explain the basic framework for constructing category hierarchy, and describe hierarchical classification. Finally, we report some experiments using the 1996 Reuters corpus with a discussion of evaluation.",,
Conclusion:,,
"We proposed a method for generating category hierarchy in order to improve text classification performance. We used 񮽙-means and a ÐÓ×× ÙÒÒØØÓÒ which is derived from NB classifiers. We found small advantages in the F-score for automatically generated hierarchy, compared with a baseline flat non-hierarchy and that of manually constructed hierarchy from large training samples. We have also shown that our method can benefit significantly from less training samples. Future work includes (i) extracting features which discriminate between categories within the same cluster with low F-score, (ii) using other machine learning techniques to obtain further advantages in efficiency in dealing with a large collection of data, (iii) comparing the method with other techniques such as hierarchical agglomerative clustering and 'X-means'), and (iv) developing evaluation method between manual and automatic construction of hierarchies to learn more about the strengths and weaknesses of the two methods of classifying documents.",,
,,
Title 1: Automatic Construction of Category Hierarchy for Improved Classification of Large Corpora,2,Text classification is the crucial and (well-knownI keyword missing from the abstract
Title 2: Automatic Construction of Category Hierarchy for Text Classification,,
,,
,,
ID: 2134,,
Abstract:,,
"In this paper, we describe our system which participated in the SemEval 2010 task of disambiguating sentiment ambiguous adjectives for Chinese. Our system uses text messages from Twitter, a popular microblogging platform, for building a dataset of emotional texts. Using the built dataset, the system classifies the meaning of adjectives into positive or negative sentiment polarity according to the given context. Our approach is fully automatic. It does not require any additional hand-built language resources and it is language independent .",,
Introduction:,,
"The dataset of the SemEval task ( consists of short texts in Chinese containing target adjectives whose sentiments need to be disambiguated in the given contexts. Those adjectives are: big, small, many, few, high, low, thick, thin, deep, shallow, heavy, light, huge, grave. Disambiguating sentiment ambiguous adjectives is a challenging task for NLP. Previous studies were mostly focused on word sense disambiguation rather than sentiment disambiguation. Although both problems look similar, the latter is more challenging in our opinion because impregnated with more subjectivity. In order to solve the task, one has to deal not only with the semantics of the context, but also with the psychological aspects of human perception of emotions from the written text.In our approach, we use Twitter 1 microblogging platform to retrieve emotional messages and form two sets of texts: messages with positive emotions and those with negative ones (Pak and Paroubek, 1 http://twitter.com 2010). We use emoticons 2 as indicators of an emotion) to automatically classify texts into positive or negative sets. The reason we use Twitter is because it allows us to collect the data with minimal supervision efforts. It provides an API 3 which makes the data retrieval process much more easier then Web based search or other resources.After the dataset of emotional texts has been obtained, we build a classifier based on n-grams Na¨ıveNa¨ıve Bayes approach. We tested two approaches to build a sentiment classifier:1. In the first one, we collected Chinese texts from Twitter and used them to train a classifier to annotate the test dataset.2. In the second one, we used machine translator to translate the dataset from Chinese to English and annotated it using collected English texts from Twitter as the training data.We have made the second approach because we were able to collect much more of English texts from Twitter than Chinese ones and we wanted to test the impact of machine translation on the performance of our classifier. We have experimented with Google Translate and Yahoo Babelfish . Google Translate yielded better results.",,
Conclusion:,,
"In this paper, we have described our system for disambiguating sentiments of adjectives in Chinese texts. Our Na¨ıveNa¨ıve Bayes approach uses information automatically extracted from Twitter microblogs using emoticons. The techniques used in our approach can be applied to any other language. Our system is fully automate and does not utilize any hand-built lexicon. We were able to achieve up to 64% of macro and 61% of micro accuracy at the SemEval 2010 taskFor the future work, we would like to collect more Chinese texts from Twitter or similar microblogging platforms. We think that increasing the training dataset will improve much the accuracy of the sentiment disambiguation.",,
,,
Title 1: Automatic Disambiguation of Chinese Sentiment Ambiguous Adjectives Using Twitter,1,2 is non-grammatical and hallucinates (NUS-CORE?)
Title 2: NUS-CORE: Using Twitter to Disambiguate Adjective Sentiment Ambiguous Adjectives,,
,,
,,
ID: 747,,
Abstract:,,
"We describe our submission to WMT 2019 News translation shared task for Gujarati-English language pair. We submit constrained systems, i.e, we rely on the data provided for this language pair and do not use any external data. We train Transformer based subword-level neural machine translation (NMT) system using original parallel corpus along with synthetic parallel corpus obtained through back-translation of monolin-gual data. Our primary systems achieve BLEU scores of 10.4 and 8.1 for Gujarati→English and English→Gujarati, respectively. We observe that incorporating monolingual data through back-translation improves the BLEU score significantly over baseline NMT and SMT systems for this language pair.",,
Introduction:,,
"In this paper, we describe the system that we submit to the WMT 2019 1 news translation shared task.We participate in Gujarati-English language pair and submit two systems: English→Gujarati and Gujarati→English. Gujarati language belongs to Indo-Aryan language family and is spoken predominantly in the Indian state of Gujarat. It is a low-resource language as only a few thousands parallel sentences are available, which are not enough to train a neural machine translation (NMT) system as well statistical machine translation (SMT) system. Gujarati-English is a distant language pair and they have different linguistic properties including syntax, morphology, word order etc. English follows subject-verb-object order while Gujarati follows subject-object-verb order. http://www.statmt.org/wmt19/ translation-task.html NMT ( has recently become dominant paradigm for machine translation (MT) achieving state-ofthe-art on standard benchmark data sets for many language pairs. As opposed to SMT, NMT systems are trained in an end-to-end manner. Training an effective NMT requires a huge amount of high-quality parallel corpus and in absence of that, an NMT system tends to perform poorly (. However, back-translation () has been shown to improve NMT systems in such a situation. In this work, we train a SMT system and an NMT system for both English→Gujarati and Gujarati→English using the original training data. SMT systems are also used to generate synthetic parallel corpora through back-translation of monolingual data from English news crawl and Gujarati Wikipedia dumps. These corpora along with the original training corpora are used to improve the baseline NMT systems. All the SMT and NMT systems are trained at subword level.Our SMT systems are standard phrase-based SMT systems (, and NMT systems are based on Transformer ( architecture. Experiments show that NMT systems achieve BLEU () scores of 10.4 and 8.1 for Gujarati→English and English→Gujarati, respectively, outperforming the baseline SMT systems even in the absence of enough-sized parallel data.Rest of the paper is arranged in following manner: Section 2 gives brief introduction of the Transformer architecture that we used for NMT training, Section 3 describes the task, Section 4 describes the submitted systems, Section 5 gives various evaluation scores for English-Gujarati translation pair, and finally, Section 6 concludes the work.",,
Conclusion:,,
"In this paper, we described our submission to the WMT 2019 News translation shared task for Gujarati-English language pair. This is the first time Gujarati language is introduced in a WMT shared task. We submit Transformer based NMT systems for English-Gujarati language pair. Since the number of parallel sentences in training set are very less and many sentences have length of only 2-3 tokens, BLEU scores for English",,
,,
Title 1: Subword-level Neural Machine Translation for Gujarati-English Language Pair,1,I think subword is a good keyword
Title 2: English-Gujarati Machine Translation System for WMT19 News Translation Task,,
,,
,,
ID: 777,,
Abstract:,,
"This paper describes a lexicon organized around systematic polysemy: a set of word senses that are related in systematic and predictable ways. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut. We compare our lexicon to WordNet cousins, and the inter-annotator disagreement observed between WordNet Semcor and DSO corpora.",,
Introduction:,,
"In recent years, the granularity of word senses for computational lexicons has been discussed frequently in Lexical Semantics (for example,). This issue emerged as a prominent problem after previous studies and exercises in Word Sense Disambiguation (WSD) reported that, when ne-grained sense deenitions such as those in WordNet were used, entries became very similar and indistinguishable to human annotators, thereby causing disagreement on correct tags). In addition to WSD, the selection of sense inventories is fundamentally critical in other Natural Language Processing (NLP) tasks such as Information Extraction (IE) and Machine Translation (MT), as well as in Information Retrieval (IR), since the diierence in the correct sense assignments aaects recall, precision and other evaluation measures.In response to this, several approaches have been proposed which group 񮽙ne-grained word senses in various ways to derive coarse-grained sense groups. Some approaches utilize an abstraction hierarchy d ened in a dictionary, while others utilize surface syntactic patterns of the functional structures (such as predicate-argument structure for verbs) of words. Also, the current version of WordNet (1.6) encodes groupings of similar/related word senses (or synsets) by a relation called cousin.Another approach to grouping word senses is to utilize a linguistic phenomenon called systematic polysemy: a set of word senses that are related in systematic and predictable ways. For example, ANIMAL and MEAT meanings of the word \chicken"" are related because chicken as meat refers to the esh of ac hicken as a bird that is used for food. This relation is systematic, since many ANIMAL words such a s \duck"" and \lamb"" have a MEAT meaning. Another example is the relation QUANTITY-PROCESS observed in nouns such as \increase"" and \supply"".Sense grouping based on systematic polysemy is lexico-semantically motivated in that it expresses general human knowledge about the relatedness of word meanings. Such sense groupings have advantages compared to other approaches. First, related senses of a word often exist simultaneously in a discourse (for example the QUANTITY and PROCESS meanings of \increase"" above). Thus, systematic polysemy can be eeectively used in WSD (and WSD evaluation) to accept multiple or alternative sense tags. Second, many systematic relations are observed between senses which belong to diierent semantic categories. So if a lexicon is deened by a collection of separate trees/hierarchies (such as the case of WordNet), systematic polysemy can express similarity between senses that are not hierarchically proximate. Third, by explicitly representing (inter-)relations between senses, a lexicon based on systematic polysemy can facilitate semantic inferences. Thus it is useful in knowledge-intensive NLP tasks such as discourse analysis, IE and MT. More recently, () also discusses potential usefulness of systematic polysemy for clustering word senses for IR.However, extracting systematic relations from large sense inventories is a diicult task. Most often, this procedure is done manually. For example, WordNet cousin relations were identiied manually by t he W ordNet lexicographers. A similar eeort was also made in the EuroWordnet project (Vossen et 1 Systematic polysemy (in the sense we use in this paper) is also referred to as regular polysemy or logical polysemy (Pustejovsky, 1 9 9 5 ) .al., 1999). The problem is not only that manual inspection of a large, complex lexicon is very timeconsuming, it is also prone to inconsistencies.In this paper, we describes a lexicon organized around systematic polysemy. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (). In our previous work), we applied this method to a small subset of WordNet nouns and showed potential applicability. In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy. We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor ( and. The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better 񮽙 values) than arbitrary sense groupings on the agreement data.",,
Conclusion:,,
"As we reported in previous sections, our tree-cut extraction method discovered 89% of the WordNet cousins. Although the precision was relatively low (50-60%), this is an encouraging result. As for the lexicon, our sense partitions consistently yielded better 񮽙 values than arbitrary sense groupings. We consider these results to be quite promising. Our data is available at www.depaul.edu/񮽙ntomuro/research/naacl-01.html.It is signiicant to note that cluster pairs and sense partitions derived in this work are domain independent. Such information is useful in broad-domain applications, or as a background lexicon) in domain speciic applications or text categorization and IR tasks. For those tasks, we an ticipate that our extraction methods maybe useful in deriving characteristics of the domains or given corpus, as well as customizing the lexical resource. This is our next future research.For other future work, we plan to investigate an automatic way of detecting and 񮽙ltering unrelated relations. We are also planning to compare our sense partitions with the systematic disagreement obtained by)'s automatic classiier.",,
,,
Title 1: A Systematic Polysemy Lexicon Based on Tree-Cut Extraction,1,"""extraction"" is good and only available in the conclusion"
Title 2: A systematic polysemy lexicon based on tree-cut,,
,,
,,
ID: 645,,
Abstract:,,
"This paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaustively parsing the Penn Treebank with the Treebank's own CFG grammar. We show how performance is dramatically affected by rule representation and tree transformations, but little by top-down vs. bottom-up strategies. We discuss grammatical saturation, including analysis of the strongly connected components of the phrasal nonterminals in the Treebank, and model how, as sentence length increases, the effective grammar rule size increases as regions of the grammar are unlocked, yielding super-cubic observed time behavior in some configurations.",,
Introduction:,,
"This paper originated from examining the empirical performance of an exhaustive active chart parser using an untransformed treebank grammar over the Penn Treebank. Our initial experiments yielded the surprising result that for many configurations empirical parsing speed was super-cubic in the sentence length. This led us to look more closely at the structure of the treebank grammar. The resulting analysis builds on the presentation of Charniak (1996), but extends it by elucidating the structure of non-terminal interrelationships in the Penn Treebank grammar. On the basis of these studies, we build simple theoretical models which closely predict observed parser performance, and, in particular, explain the originally observed super-cubic behavior.We used treebank grammars induced directly from the local trees of the entire WSJ section of the Penn Treebank () (release 3). For each length and parameter setting, 25 sentences evenly distributed through the treebank were parsed. Since we were parsing sentences from among those from which our grammar was derived, coverage was never an issue. Every sentence parsed had at least one parse -the parse with which it was originally observed. The sentences were parsed using an implementation of the probabilistic chart-parsing algorithm presented in. In that paper, we present a theoretical analysis showing anThe default settings are shown above in boldface.We do not discuss all possible combinations of these settings. Rather, we take the bottom-up parser using an untransformed grammar with trie rule encodings to be the basic form of the parser. Except where noted, we will discuss how each factor affects this baseline, as most of the effects are orthogonal. When we name a setting, any omitted parameters are assumed to be the defaults.",,
Conclusion:,,
"We built simple but accurate models on the basis of two observations. First, passive saturation is relatively constant in span size, but large due to high reachability among phrasal categories in the grammar. Second, active saturation grows with span size because, as spans increase, the tags in a given active edge are more likely to find a matching arrangement over a span. Combining these models, we demonstrated that a wide range of empirical qualitative and quantitative behaviors of an exhaustive parser could be derived, including the potential super-cubic traversal growth over sentence lengths of interest.",,
,,
Title 1: Parsing the Penn Treebank with an Untransformed Chart Parser,2,"both titles are not good, but the 1 is too unspecific for this paper"
Title 2: Grammatical saturation in chart parsing: empirical studies and theoretical models,,
,,
,,
ID: 1022,,
Abstract:,,
"Domain adaptation is an important research topic in sentiment analysis area. Existing domain adaptation methods usually transfer sentiment knowledge from only one source domain to target domain. In this paper, we propose anew domain adaptation approach which can exploit sentiment knowledge from multiple source domains. We first extract both global and domain-specific sentiment knowledge from the data of multiple source domains using multi-task learning. Then we transfer them to target domain with the help of words' sentiment polarity relations extracted from the un-labeled target domain data. The similarities between target domain and different source domains are also incorporated into the adaptation process. Experimental results on benchmark dataset show the effectiveness of our approach in improving cross-domain sentiment classification performance .",,
Introduction:,,
"Sentiment classification is a hot research topic in natural language processing field, and has many applications in both academic and industrial areas (. Sentiment classification is widely known as a domain-dependent task). The sentiment classifier trained in one domain may not perform well in another domain. This is because sentiment expressions used in different domains are usually different. For example, ""boring"" * Corresponding author. and ""lengthy"" are frequently used to express negative sentiments in Book domain. However, they rarely appear in Electronics domain). Thus a sentiment classifier trained in Electronics domain cannot accurately predict their sentiments in Book domain. In addition, the same word may convey different sentiments in different domains. For example, in Electronics domain ""easy"" is usually used in positive reviews, e.g., ""this digital camera is easy to use."" However, it is frequently used as a negative word in Movie domain. For instance, ""the ending of this movie is easy to guess."" Thus, the sentiment classifier trained in one domain usually cannot be applied to another domain directly.In order to tackle this problem, sentiment domain adaptation has been widely studied. For example, proposed to compute the correspondence among features from different domains using their associations with pivot features based on structural correspondence learning (SCL). proposed a spectral feature alignment (SFA) algorithm to align the domain-specific words from different domains in order to reduce the gap between source and target domains. However, all of these methods transfer sentiment information from only one source domain. When the source and target domains have significant difference in feature distributions, the adaptation performance will heavily decline. In some cases, the performance of sentiment domain adaptation is even worse than that without adaptation, which is usually known as negative transfer .In this paper we propose anew domain adaptation approach for cross-domain sentiment classification. Our approach can exploit the sentiment information in multiple source domains to reduce the risk of negative transfer effectively. Our approach consists of two steps, i.e., training and adaptation. At the training stage, we extract two kinds of sentiment models, i.e., the global model and the domain-specific models, from the data of multiple source domains using multi-task learning. The global sentiment model can capture the common sentiment knowledge shared by various domains, and has better generalization performance than the sentiment model trained in a single source domain. The domain-specific sentiment model can capture the specific sentiment knowledge in each source domain. At the adaptation stage, we transfer both kinds of sentiment knowledge to target domain with the help of the words' sentiment graph of target domain. The sentiment graph contains words' domain-specific sentiment polarity relations extracted from the syntactic parsing results of the unlabeled data in target domain. Since sentiment transfer between similar domains is more effective than dissimilar domains, we incorporate the similarities between target domain and different source domains into the adaptation process. In order to estimate the similarity between two domains, we propose a novel domain similarity measure based on their sentiment graphs. Extensive experiments were conducted on the benchmark Amazon product review dataset. The experimental results show that our approach can improve the performance of cross-domain sentiment classification effectively.",,
Conclusion:,,
"This paper presents a sentiment domain adaptation approach which transfers the sentiment knowledge from multiple source domains to target domain. Our approach consists of two steps. First, we extract both global and domain-specific sentiment knowledge from the data of multiple source domains. Second, we transfer these two kinds of sentiment knowledge to target domain with the help of the words' sentiment graph. We proposed to build words' sentiment graph for target domain by extracting their sentiment polarity relations from massive unlabeled data. Besides, we proposed a novel domain similarity measure based on sentiment graphs, and incorporated the domain similarities between target and different source domains into the domain adaptation process. The experimental results on a benchmark dataset show that our approach can effectively improve the performance of cross-domain sentiment classification.",,
,,
Title 1: Sentiment Domain Adaptation for Cross-Domain Sentiment Classification,2,"""multi-task"" looks like a good keyword"
Title 2: Multi-Task Domain Adaptation for Cross-Domain Sentiment Classification,,
,,
,,
ID: 1697,,
Abstract:,,
"Certain distinctions made in the lexicon of one language maybe redundant when translating into another language. We quantify redundancy among source types by the similarity of their distributions over target types. We propose a language-independent framework for minimising lexical redundancy that can be optimised directly from parallel text. Optimisation of the source lexicon fora given target language is viewed as model selection over a set of cluster-based translation models. Redundant distinctions between types may exhibit monolingual regularities, for example , inflexion patterns. We define a prior over model structure using a Markov random field and learn features over sets of monolingual types that are predictive of bilingual redundancy. The prior makes model selection more robust without the need for language-specific assumptions regarding redundancy. Using these models in a phrase-based SMT system, we show significant improvements in translation quality for certain language pairs.",,
Introduction:,,
"Data-driven machine translation (MT) relies on models that can be efficiently estimated from parallel text. Token-level independence assumptions based on word-alignments can be used to decompose parallel corpora into manageable units for parameter estimation. However, if training data is scarce or language pairs encode significantly different information in the lexicon, such as Czech and English, additional independence assumptions may assist the model estimation process.Standard statistical translation models use separate parameters for each pair of source and target types. In these models, distinctions in either lexicon that are redundant to the translation process will result in unwarranted model complexity and make parameter estimation from limited parallel data more difficult. A natural way to eliminate such lexical redundancy is to group types into homogeneous clusters that do not differ significantly in their distributions over types in the other language. Cluster-based translation models capture the corresponding independence assumptions.Previous work on bilingual clustering has focused on coarse partitions of the lexicon that resemble automatically induced part-of-speech classes. These were used to model generic word-alignment patterns such as noun-adjective re-ordering between. In contrast, we induce fine-grained partitions of the lexicon, conceptually closer to automatic lemmatisation, optimised specifically to assign translation probabilities. Unlike lemmatisation or stemming, our method specifically quantifies lexical redundancy in a bilingual setting and does not make language-specific assumptions.We tackle the problem of redundancy in the translation lexicon via Bayesian model selection over a set of cluster-based translation models. We search for the model, defined by a clustering of the source lexicon, that maximises the marginal likelihood of target tokens in parallel data. In this optimisation, source types are combined into clusters if their distributions over target types are too similar to warrant distinct parameters.Redundant distinctions between types may exhibit regularities within a language, for instance, inflexion patterns. These can be used to guide model selection. Here we show that the inclusion of a model 'prior' over the lexicon structure leads to more robust translation models. Although a priori we do not know which monolingual features characterise redundancy fora given language pair, by defining a model over the prior monolingual space of source types and cluster assignments, we can introduce an inductive bias that allows clustering decisions in different parts of the lexicon to influence one another via monolingual features. We use an EM-type algorithm to learn weights fora Markov random field parameterisation of this prior over lexicon structure.We obtain significant improvements in translation quality as measured by BLEU, incorporating these optimised model within a phrase-based SMT system for three different language pairs. The MRF prior improves the results and picks up features that appear to agree with linguistic intuitions of redundancy for the language pairs considered.",,
Conclusion:,,
"We proposed a framework for modelling lexical redundancy in machine translation and tackled optimisation of the lexicon via Bayesian model selection over a set of cluster-based translation models. We showed improvements in translation quality incorporating these models within a phrasebased SMT sytem. Additional gains resulted from the inclusion of an MRF prior over model structure. We demonstrated that this prior could be used to learn weights for monolingual features that characterise bilingual redundancy. Preliminary experiments defining MRF features over morphological annotation suggest this model can also identify redundant distinctions categorised linguistically (for instance, that morphological case is redundant on Czech nouns and adjectives with respect to English, while number is redundant only on adjectives). In future work we will investigate the use of linguistic resources to define feature sets for the MRF prior. Lexical redundancy would ideally be addressed in the context of phrases, however, computation and statistical estimation may then be significantly more challenging.",,
,,
Title 1: Modelling Lexical Redundancy in Statistical Machine Translation,2,"both minimising and modelling are rare in the abstract, intro and conclusion. I give priority to the abstract, then"
Title 2: Minimising Lexical Redundancy in Statistical Machine Translation,,
,,
,,
ID: 1817,,
Abstract:,,
"We consider anew subproblem of unsuper-vised parsing from raw text, unsupervised partial parsing-the unsupervised version of text chunking. We show that addressing this task directly, using probabilistic finite-state methods , produces better results than relying on the local predictions of a current best unsu-pervised parser, Seginer's (2007) CCL. These finite-state models are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally , we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries , both in our system and in CCL.",,
Introduction:,,
"Unsupervised grammar induction has been an active area of research in computational linguistics for over twenty years (. Recent work) has largely built on the dependency model with valence of, and is characterized by its reliance on gold-standard part-of-speech (POS) annotations: the models are trained on and evaluated using sequences of POS tags rather than raw tokens. This is also true for models which are not successors of).An exception which learns from raw text and makes no use of POS tags is the common cover links parser. CCL established stateof-the-art results for unsupervised constituency parsing from raw text, and it is also incremental and extremely fast for both learning and parsing. Unfortunately, CCL is a non-probabilistic algorithm based on a complex set of inter-relating heuristics and a non-standard (though interesting) representation of constituent trees. This makes it hard to extend. Note that although improve on Seginer's results, they do so by selecting training sets to best match the particular test sentences-CCL itself is used without modification. explore an alternative strategy of unsupervised partial parsing: directly predicting low-level constituents based solely on word co-occurrence frequencies. Essentially, this means segmenting raw text into multiword constituents. In that paper, we show-somewhat surprisingly-that CCL's performance is mostly dependent on its effectiveness at identifying low-level constituents. In fact, simply extracting non-hierarchical multiword constituents from CCL's output and putting a rightbranching structure over them actually works better than CCL's own higher level predictions. This result suggests that improvements to low-level constituent prediction will ultimately lead to further gains in overall constituent parsing.Here, we present such an improvement by using probabilistic finite-state models for phrasal segmentation from raw text. The task for these models is chunking, so we evaluate performance on identification of multiword chunks of all constituent types as well as only noun phrases. Our unsupervised chunkers extend straightforwardly to a cascade that predicts higher levels of constituent structure, similar to the supervised approach of. This forms an overall unsupervised parsing system that outperforms CCL by a wide margin.Mrs.",,
Conclusion:,,
"In this paper we have introduced anew subproblem of unsupervised parsing: unsupervised partial parsing, or unsupervised chunking. We have proposed a model for unsupervised chunking from raw text that is based on standard probabilistic finitestate methods. This model produces better local constituent predictions than the current best unsupervised parser, CCL, across datasets in English, German, and Chinese. By extending these probabilistic finite-state methods in a cascade, we obtain a general unsupervised parsing model. This model outperforms CCL in PARSEVAL evaluation on English, German, and Chinese.Like CCL, our models operate from raw (albeit segmented) text, and like it our models decode very quickly; however, unlike CCL, our models are based on standard and well-understood computational linguistics technologies (hidden Markov models and related formalisms), and may benefit from new research into these core technologies. For instance, our models maybe improved by the application of (unsupervised) discriminative learning techniques with features; or by incorporating topic models and document information (. UPPARSE, the software used for the experiments in this paper, is available under an open-source license to facilitate replication and extensions.",,
,,
Title 1: Unsupervised Partial Parsing with Probabilistic Finite State Models,1,"""partial parsing with p"" - a nice alliteration"
Title 2: Probabilistic Finite-State Models for Unsupervised Chunking,,
,,
,,
ID: 342,,
Abstract:,,
"We present a simple log-linear reparame-terization of IBM Model 2 that overcomes problems arising from Model 1's strong assumptions and Model 2's overparame-terization. Efficient inference, likelihood evaluation, and parameter estimation algorithms are provided. Training the model is consistently ten times faster than Model 4. On three large-scale translation tasks, systems built using our alignment model outperform IBM Model 4. An open-source implementation of the alignment model described in this paper is available from http://github.com/clab/fast align .",,
Introduction:,,
"Word alignment is a fundamental problem in statistical machine translation. While the search for more sophisticated models that provide more nuanced explanations of parallel corpora is a key research activity, simple and effective models that scale well are also important. These play a crucial role in many scenarios such as parallel data mining and rapid large scale experimentation, and as subcomponents of other models or training and inference algorithms. For these reasons, IBM Models 1 and 2, which support exact inference in time Θ(|f| · |e|), continue to be widely used.This paper argues that both of these models are suboptimal, even in the space of models that permit such computationally cheap inference. Model 1 assumes all alignment structures are uniformly likely (a problematic assumption, particularly for frequent word types), and Model 2 is vastly overparameterized, making it prone to degenerate behavior on account of overfitting. We present a simple log-linear reparameterization of Model 2 that avoids both problems ( §2). While inference in log-linear models is generally computationally more expensive than in their multinomial counterparts, we show how the quantities needed for alignment inference, likelihood evaluation, and parameter estimation using EM and related methods can be computed using two simple algebraic identities ( §3), thereby defusing this objection. We provide results showing our model is an order of magnitude faster to train than Model 4, that it requires no staged initialization, and that it produces alignments that lead to significantly better translation quality on downstream translation tasks ( §4).",,
Conclusion:,,
"We have presented a fast and effective reparameterization of IBM Model 2 that is a compelling replacement for for the standard Model 4. Although the alignment quality results measured in terms of AER are mixed, the alignments were shown to work exceptionally well in downstream translation systems on a variety of language pairs.",,
,,
Title 1: Fast Alignment in IBM Models: Fast Alignment for Statistical Machine Translation,2,"the repetition in ""1"" is undesirable"
Title 2: Fast Alignment for Statistical Machine Translation,,
,,
,,
ID: 1843,,
Abstract:,,
"One of the challenges of building natural language processing (NLP) applications for education is finding a large domain-specific corpus for the subject of interest (e.g., history or science). To address this challenge, we propose a tool, Dexter, that extracts a subject-specific corpus from a heterogeneous corpus, such as Wikipedia, by relying on a small seed corpus and distributed document representations. We empirically show the impact of the generated corpus on language modeling, estimating word embeddings, and consequently, distractor generation, resulting in a better performance than while using a general domain corpus, a heuristically constructed domain-specific corpus, and a corpus generated by a popular system: BootCaT.",,
Introduction:,,
"Educational applications tend to target a specific subject, in other words, a specific domain, such as the medical domain in the case of). Thus, building these applications with underlying NLP algorithms, would typically require a large domain-specific corpus. Example uses of these large corpora are estimating language models), estimating word embeddings (, and estimating document embeddings (). These estimations are central to several downstream applications including automatic speech recognition, machine translation (, and text categorization (.Previous findings, such as, have shown that training NLP applications on a domain different from the target domain could prove detrimental to the performance of these applications. In order to help educational applications in specific disciplines such as science and history create a large, yet domain-specific corpus, we propose a domain extraction tool, Dexter , that extracts a domain-specific corpus from Wikipedia.The algorithm, elaborated in Section 2, retrieves a set of documents from Wikipedia that are closest indiscipline to a user-supplied small seed corpus. The size of this extracted set is a user-defined hyperparameter, and thus controls the trade-off between the specificity of the output corpus and its size. We empirically determine the favorable configuration of Dexter, demonstrate its benefits towards estimating word embeddings, and consequently distractor generation, as well as language models. We also show how, on the aformentioned tasks, Dexter outperforms BootCaT, a popular toolkit to automatically create an Internet-derived corpus (). Datasets used in this research are released for public use 2 .",,
Conclusion:,,
"Relying on off-the-shelf resources reduces the quality of educational NLP applications. To address this challenge, we offer to the community an aiding tool, Dexter, to extract a domainspecific corpus from Wikipedia. We show that our simple method outperforms in-domain corpora constructed heuristically using Wikipedia's taxonomy, or those constructed using popular systems scraping the World Wide Web.",,
,,
Title 1: Dexter: Extracting domain-specific corpus for education,1,"While ""1"" is not good, it is better than 2 because it includes the crucial keyword ""Education"""
Title 2: Extracting a Domain-Specific Corpus from Wikipedia,,